{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee-thien-tuyen/PCB_faults_extract/blob/main/train_resnet101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1B984Vb0ddx",
        "outputId": "4196c1fe-9e0d-45af-af45-129b75037616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9werZVNO0nmv"
      },
      "outputs": [],
      "source": [
        "!unrar x \"/content/drive/MyDrive/Train_PCB_faults/PCBData.rar\" \"/content/drive/MyDrive/Train_PCB_faults/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hTwPI3Aayzp2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "CLASSES = [\n",
        "    \"open\", \"short\", \"mousebit\",\n",
        "    \"spur\", \"copper\", \"pin-hole\"\n",
        "]\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Train_PCB_faults/PCBData\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Train_PCB_faults/output\"\n",
        "DEFECTS_PATH = os.path.sep.join([DATASET_PATH, \"defects\"])\n",
        "TEST_PATH = os.path.sep.join([DATASET_PATH, \"test_defects\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE3twE7Gy68S"
      },
      "outputs": [],
      "source": [
        "os.makedirs(DEFECTS_PATH, exist_ok=True)\n",
        "for catigorie in CLASSES:\n",
        "    os.makedirs(os.path.sep.join([DEFECTS_PATH, catigorie]), exist_ok=True)\n",
        "\n",
        "for group in os.listdir(DATASET_PATH):\n",
        "    if group.startswith('group'):\n",
        "        print(f\"[INFO] Extracted defects from {group}\")\n",
        "        group_images_path = os.path.sep.join([DATASET_PATH, group, group[5:]])\n",
        "        group_annotations_path = f\"{group_images_path}_not\"\n",
        "        for annotation in os.listdir(group_annotations_path):\n",
        "            image = cv2.imread(os.path.sep.join([group_images_path, f\"{annotation[:-4]}_test.jpg\"]))\n",
        "            with open(os.path.sep.join([group_annotations_path, annotation]), 'r') as f:\n",
        "                defects_data = f.read()\n",
        "                for i, line in enumerate(defects_data.splitlines()):\n",
        "                    (x1, y1, x2, y2, c) = map(lambda t: int(t), line.split(' '))\n",
        "                    label = CLASSES[c-1]\n",
        "                    region = image[y1:y2, x1:x2]\n",
        "                    cv2.imwrite(os.path.sep.join([DEFECTS_PATH, label, f\"defect_{annotation[:-4]}_{i}.jpg\"]), region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uvjPnyqQyDRq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet101\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import glob\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WubEoI9JyDRs"
      },
      "outputs": [],
      "source": [
        "datagenAug = ImageDataGenerator(\n",
        "    rotation_range=90,\n",
        "\tzoom_range=0.15,\n",
        "\thorizontal_flip=True,\n",
        "\tvertical_flip=True,\n",
        "\tvalidation_split=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VrmzuXEyDRt",
        "outputId": "968cecf5-d44e-4b5b-bbfa-dc0d377d4491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8012 images belonging to 6 classes.\n",
            "Found 2001 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "trainGen = datagenAug.flow_from_directory(\n",
        "    DEFECTS_PATH, classes=CLASSES,\n",
        "    target_size=(224, 224), class_mode=\"categorical\",\n",
        "    batch_size=32, subset=\"training\")\n",
        "\n",
        "testGen = datagenAug.flow_from_directory(\n",
        "    DEFECTS_PATH, classes=CLASSES,\n",
        "    target_size=(224, 224), class_mode=\"categorical\",\n",
        "    batch_size=32, subset=\"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYD0ysz-yDRv",
        "outputId": "d937d5d6-6142-4c83-e86f-72c2d21ff10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171446536/171446536 [==============================] - 9s 0us/step\n"
          ]
        }
      ],
      "source": [
        "resnet_model = ResNet101(weights='imagenet', include_top=False,  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "for layer in resnet_model.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "head = resnet_model.output\n",
        "head = AveragePooling2D(pool_size=(7, 7))(head)\n",
        "flatten = Flatten()(head)\n",
        "fc = Dense(512, activation = \"relu\")(flatten)\n",
        "fc = Dropout(0.5)(fc)\n",
        "output = Dense(len(trainGen.class_indices), activation = \"softmax\")(fc)\n",
        "\n",
        "model = Model(inputs=resnet_model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HR7JIQt2yDRw"
      },
      "outputs": [],
      "source": [
        "opt = Adam(learning_rate=1e-4)\n",
        "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KWiwpjeIyDRw"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(os.path.sep.join([OUTPUT_PATH, \"resnet101.h5\"]), monitor='accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n",
        "early = EarlyStopping(monitor='accuracy', min_delta=0, patience=100, verbose=1, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X-wIAELqyDRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588bf789-6461-4ca2-8ac6-91d53bccc2f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\n",
            "Epoch 1: accuracy improved from -inf to 0.15625, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  1/250 [..............................] - ETA: 3:24:02 - loss: 2.9014 - accuracy: 0.1562\n",
            "Epoch 1: accuracy improved from 0.15625 to 0.17188, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  2/250 [..............................] - ETA: 2:13:53 - loss: 2.5416 - accuracy: 0.1719\n",
            "Epoch 1: accuracy improved from 0.17188 to 0.17708, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  3/250 [..............................] - ETA: 2:01:54 - loss: 2.4600 - accuracy: 0.1771\n",
            "Epoch 1: accuracy improved from 0.17708 to 0.17969, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  4/250 [..............................] - ETA: 2:02:04 - loss: 2.6064 - accuracy: 0.1797\n",
            "Epoch 1: accuracy improved from 0.17969 to 0.18750, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  5/250 [..............................] - ETA: 1:58:08 - loss: 2.6168 - accuracy: 0.1875\n",
            "Epoch 1: accuracy did not improve from 0.18750\n",
            "  6/250 [..............................] - ETA: 1:51:33 - loss: 2.5918 - accuracy: 0.1823\n",
            "Epoch 1: accuracy improved from 0.18750 to 0.20089, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  7/250 [..............................] - ETA: 1:53:38 - loss: 2.5521 - accuracy: 0.2009\n",
            "Epoch 1: accuracy improved from 0.20089 to 0.21094, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  8/250 [..............................] - ETA: 1:52:25 - loss: 2.5244 - accuracy: 0.2109\n",
            "Epoch 1: accuracy improved from 0.21094 to 0.21181, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  9/250 [>.............................] - ETA: 1:50:33 - loss: 2.5090 - accuracy: 0.2118\n",
            "Epoch 1: accuracy improved from 0.21181 to 0.22187, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 10/250 [>.............................] - ETA: 1:50:16 - loss: 2.4432 - accuracy: 0.2219\n",
            "Epoch 1: accuracy improved from 0.22187 to 0.23295, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 11/250 [>.............................] - ETA: 1:46:55 - loss: 2.3930 - accuracy: 0.2330\n",
            "Epoch 1: accuracy improved from 0.23295 to 0.24479, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 12/250 [>.............................] - ETA: 1:46:30 - loss: 2.3664 - accuracy: 0.2448\n",
            "Epoch 1: accuracy improved from 0.24479 to 0.25240, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 13/250 [>.............................] - ETA: 1:44:29 - loss: 2.3223 - accuracy: 0.2524\n",
            "Epoch 1: accuracy improved from 0.25240 to 0.26339, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 14/250 [>.............................] - ETA: 1:42:52 - loss: 2.2896 - accuracy: 0.2634\n",
            "Epoch 1: accuracy improved from 0.26339 to 0.26875, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 15/250 [>.............................] - ETA: 1:42:45 - loss: 2.2764 - accuracy: 0.2688\n",
            "Epoch 1: accuracy improved from 0.26875 to 0.27148, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 16/250 [>.............................] - ETA: 1:40:58 - loss: 2.2418 - accuracy: 0.2715\n",
            "Epoch 1: accuracy did not improve from 0.27148\n",
            " 17/250 [=>............................] - ETA: 1:39:52 - loss: 2.2323 - accuracy: 0.2702\n",
            "Epoch 1: accuracy improved from 0.27148 to 0.28299, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 18/250 [=>............................] - ETA: 1:40:27 - loss: 2.1864 - accuracy: 0.2830\n",
            "Epoch 1: accuracy improved from 0.28299 to 0.28947, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 19/250 [=>............................] - ETA: 1:39:41 - loss: 2.1579 - accuracy: 0.2895\n",
            "Epoch 1: accuracy improved from 0.28947 to 0.29844, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 20/250 [=>............................] - ETA: 1:39:15 - loss: 2.1278 - accuracy: 0.2984\n",
            "Epoch 1: accuracy improved from 0.29844 to 0.30060, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 21/250 [=>............................] - ETA: 1:39:19 - loss: 2.1046 - accuracy: 0.3006\n",
            "Epoch 1: accuracy improved from 0.30060 to 0.30398, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 22/250 [=>............................] - ETA: 1:39:06 - loss: 2.0869 - accuracy: 0.3040\n",
            "Epoch 1: accuracy improved from 0.30398 to 0.31250, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 23/250 [=>............................] - ETA: 1:38:46 - loss: 2.0436 - accuracy: 0.3125\n",
            "Epoch 1: accuracy improved from 0.31250 to 0.32031, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 24/250 [=>............................] - ETA: 1:38:04 - loss: 2.0242 - accuracy: 0.3203\n",
            "Epoch 1: accuracy improved from 0.32031 to 0.32250, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 25/250 [==>...........................] - ETA: 1:37:45 - loss: 2.0024 - accuracy: 0.3225\n",
            "Epoch 1: accuracy improved from 0.32250 to 0.32452, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 26/250 [==>...........................] - ETA: 1:37:30 - loss: 1.9850 - accuracy: 0.3245\n",
            "Epoch 1: accuracy improved from 0.32452 to 0.32870, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 27/250 [==>...........................] - ETA: 1:36:34 - loss: 1.9619 - accuracy: 0.3287\n",
            "Epoch 1: accuracy improved from 0.32870 to 0.33705, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 28/250 [==>...........................] - ETA: 1:36:18 - loss: 1.9340 - accuracy: 0.3371\n",
            "Epoch 1: accuracy improved from 0.33705 to 0.34159, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 29/250 [==>...........................] - ETA: 1:35:29 - loss: 1.9144 - accuracy: 0.3416\n",
            "Epoch 1: accuracy improved from 0.34159 to 0.34688, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 30/250 [==>...........................] - ETA: 1:35:22 - loss: 1.8937 - accuracy: 0.3469\n",
            "Epoch 1: accuracy improved from 0.34688 to 0.35484, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 31/250 [==>...........................] - ETA: 1:34:47 - loss: 1.8699 - accuracy: 0.3548\n",
            "Epoch 1: accuracy improved from 0.35484 to 0.36133, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 32/250 [==>...........................] - ETA: 1:34:01 - loss: 1.8422 - accuracy: 0.3613\n",
            "Epoch 1: accuracy improved from 0.36133 to 0.36742, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 33/250 [==>...........................] - ETA: 1:33:32 - loss: 1.8174 - accuracy: 0.3674\n",
            "Epoch 1: accuracy improved from 0.36742 to 0.37316, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 34/250 [===>..........................] - ETA: 1:33:19 - loss: 1.7953 - accuracy: 0.3732\n",
            "Epoch 1: accuracy improved from 0.37316 to 0.38125, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 35/250 [===>..........................] - ETA: 1:32:40 - loss: 1.7715 - accuracy: 0.3812\n",
            "Epoch 1: accuracy improved from 0.38125 to 0.38368, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 36/250 [===>..........................] - ETA: 1:32:29 - loss: 1.7530 - accuracy: 0.3837\n",
            "Epoch 1: accuracy improved from 0.38368 to 0.38514, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 37/250 [===>..........................] - ETA: 1:31:49 - loss: 1.7439 - accuracy: 0.3851\n",
            "Epoch 1: accuracy did not improve from 0.38514\n",
            " 38/250 [===>..........................] - ETA: 1:31:08 - loss: 1.7369 - accuracy: 0.3849\n",
            "Epoch 1: accuracy improved from 0.38514 to 0.39423, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 39/250 [===>..........................] - ETA: 1:30:54 - loss: 1.7130 - accuracy: 0.3942\n",
            "Epoch 1: accuracy improved from 0.39423 to 0.40234, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 40/250 [===>..........................] - ETA: 1:30:23 - loss: 1.6910 - accuracy: 0.4023\n",
            "Epoch 1: accuracy improved from 0.40234 to 0.40625, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 41/250 [===>..........................] - ETA: 1:29:57 - loss: 1.6791 - accuracy: 0.4062\n",
            "Epoch 1: accuracy did not improve from 0.40625\n",
            " 42/250 [====>.........................] - ETA: 1:29:19 - loss: 1.6733 - accuracy: 0.4055\n",
            "Epoch 1: accuracy improved from 0.40625 to 0.41206, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 43/250 [====>.........................] - ETA: 1:29:07 - loss: 1.6537 - accuracy: 0.4121\n",
            "Epoch 1: accuracy improved from 0.41206 to 0.41832, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 44/250 [====>.........................] - ETA: 1:28:22 - loss: 1.6375 - accuracy: 0.4183\n",
            "Epoch 1: accuracy improved from 0.41832 to 0.41944, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 45/250 [====>.........................] - ETA: 1:27:58 - loss: 1.6283 - accuracy: 0.4194\n",
            "Epoch 1: accuracy improved from 0.41944 to 0.42255, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 46/250 [====>.........................] - ETA: 1:27:30 - loss: 1.6138 - accuracy: 0.4226\n",
            "Epoch 1: accuracy improved from 0.42255 to 0.42620, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 47/250 [====>.........................] - ETA: 1:27:13 - loss: 1.6013 - accuracy: 0.4262\n",
            "Epoch 1: accuracy improved from 0.42620 to 0.42839, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 48/250 [====>.........................] - ETA: 1:26:47 - loss: 1.5916 - accuracy: 0.4284\n",
            "Epoch 1: accuracy improved from 0.42839 to 0.43304, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 49/250 [====>.........................] - ETA: 1:26:21 - loss: 1.5771 - accuracy: 0.4330\n",
            "Epoch 1: accuracy improved from 0.43304 to 0.43562, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 50/250 [=====>........................] - ETA: 1:25:47 - loss: 1.5669 - accuracy: 0.4356\n",
            "Epoch 1: accuracy improved from 0.43562 to 0.43811, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 51/250 [=====>........................] - ETA: 1:25:15 - loss: 1.5576 - accuracy: 0.4381\n",
            "Epoch 1: accuracy improved from 0.43811 to 0.44231, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 52/250 [=====>........................] - ETA: 1:24:33 - loss: 1.5503 - accuracy: 0.4423\n",
            "Epoch 1: accuracy improved from 0.44231 to 0.44693, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 53/250 [=====>........................] - ETA: 1:24:02 - loss: 1.5379 - accuracy: 0.4469\n",
            "Epoch 1: accuracy improved from 0.44693 to 0.45139, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 54/250 [=====>........................] - ETA: 1:23:42 - loss: 1.5266 - accuracy: 0.4514\n",
            "Epoch 1: accuracy improved from 0.45139 to 0.45511, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 55/250 [=====>........................] - ETA: 1:23:18 - loss: 1.5149 - accuracy: 0.4551\n",
            "Epoch 1: accuracy improved from 0.45511 to 0.46150, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 56/250 [=====>........................] - ETA: 1:22:42 - loss: 1.4984 - accuracy: 0.4615\n",
            "Epoch 1: accuracy improved from 0.46150 to 0.46820, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 57/250 [=====>........................] - ETA: 1:22:19 - loss: 1.4794 - accuracy: 0.4682\n",
            "Epoch 1: accuracy improved from 0.46820 to 0.47091, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 58/250 [=====>........................] - ETA: 1:22:03 - loss: 1.4704 - accuracy: 0.4709\n",
            "Epoch 1: accuracy improved from 0.47091 to 0.47564, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 59/250 [======>.......................] - ETA: 1:21:39 - loss: 1.4603 - accuracy: 0.4756\n",
            "Epoch 1: accuracy improved from 0.47564 to 0.47917, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 60/250 [======>.......................] - ETA: 1:21:16 - loss: 1.4480 - accuracy: 0.4792\n",
            "Epoch 1: accuracy improved from 0.47917 to 0.48002, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 61/250 [======>.......................] - ETA: 1:20:43 - loss: 1.4432 - accuracy: 0.4800\n",
            "Epoch 1: accuracy improved from 0.48002 to 0.48337, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 62/250 [======>.......................] - ETA: 1:20:22 - loss: 1.4360 - accuracy: 0.4834\n",
            "Epoch 1: accuracy improved from 0.48337 to 0.48363, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 63/250 [======>.......................] - ETA: 1:20:00 - loss: 1.4319 - accuracy: 0.4836\n",
            "Epoch 1: accuracy improved from 0.48363 to 0.48730, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 64/250 [======>.......................] - ETA: 1:19:35 - loss: 1.4208 - accuracy: 0.4873\n",
            "Epoch 1: accuracy improved from 0.48730 to 0.49087, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 65/250 [======>.......................] - ETA: 1:19:10 - loss: 1.4138 - accuracy: 0.4909\n",
            "Epoch 1: accuracy improved from 0.49087 to 0.49242, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 66/250 [======>.......................] - ETA: 1:18:34 - loss: 1.4090 - accuracy: 0.4924\n",
            "Epoch 1: accuracy improved from 0.49242 to 0.49440, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 67/250 [=======>......................] - ETA: 1:18:10 - loss: 1.4038 - accuracy: 0.4944\n",
            "Epoch 1: accuracy improved from 0.49440 to 0.49724, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 68/250 [=======>......................] - ETA: 1:17:45 - loss: 1.3954 - accuracy: 0.4972\n",
            "Epoch 1: accuracy improved from 0.49724 to 0.49955, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 69/250 [=======>......................] - ETA: 1:17:12 - loss: 1.3879 - accuracy: 0.4995\n",
            "Epoch 1: accuracy improved from 0.49955 to 0.50134, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 70/250 [=======>......................] - ETA: 1:16:55 - loss: 1.3790 - accuracy: 0.5013\n",
            "Epoch 1: accuracy improved from 0.50134 to 0.50352, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 71/250 [=======>......................] - ETA: 1:16:22 - loss: 1.3697 - accuracy: 0.5035\n",
            "Epoch 1: accuracy improved from 0.50352 to 0.50651, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 72/250 [=======>......................] - ETA: 1:15:54 - loss: 1.3613 - accuracy: 0.5065\n",
            "Epoch 1: accuracy improved from 0.50651 to 0.50942, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 73/250 [=======>......................] - ETA: 1:15:30 - loss: 1.3546 - accuracy: 0.5094\n",
            "Epoch 1: accuracy improved from 0.50942 to 0.51098, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 74/250 [=======>......................] - ETA: 1:14:53 - loss: 1.3493 - accuracy: 0.5110\n",
            "Epoch 1: accuracy improved from 0.51098 to 0.51250, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 75/250 [========>.....................] - ETA: 1:14:28 - loss: 1.3451 - accuracy: 0.5125\n",
            "Epoch 1: accuracy improved from 0.51250 to 0.51439, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 76/250 [========>.....................] - ETA: 1:13:49 - loss: 1.3377 - accuracy: 0.5144\n",
            "Epoch 1: accuracy improved from 0.51439 to 0.51664, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 77/250 [========>.....................] - ETA: 1:13:16 - loss: 1.3319 - accuracy: 0.5166\n",
            "Epoch 1: accuracy improved from 0.51664 to 0.51683, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 78/250 [========>.....................] - ETA: 1:12:53 - loss: 1.3308 - accuracy: 0.5168\n",
            "Epoch 1: accuracy improved from 0.51683 to 0.51780, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 79/250 [========>.....................] - ETA: 1:12:30 - loss: 1.3280 - accuracy: 0.5178\n",
            "Epoch 1: accuracy improved from 0.51780 to 0.52031, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 80/250 [========>.....................] - ETA: 1:12:01 - loss: 1.3221 - accuracy: 0.5203\n",
            "Epoch 1: accuracy improved from 0.52031 to 0.52122, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 81/250 [========>.....................] - ETA: 1:11:36 - loss: 1.3180 - accuracy: 0.5212\n",
            "Epoch 1: accuracy improved from 0.52122 to 0.52477, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 82/250 [========>.....................] - ETA: 1:11:21 - loss: 1.3107 - accuracy: 0.5248\n",
            "Epoch 1: accuracy improved from 0.52477 to 0.52711, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 83/250 [========>.....................] - ETA: 1:10:57 - loss: 1.3038 - accuracy: 0.5271\n",
            "Epoch 1: accuracy improved from 0.52711 to 0.53051, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 84/250 [=========>....................] - ETA: 1:10:33 - loss: 1.2963 - accuracy: 0.5305\n",
            "Epoch 1: accuracy improved from 0.53051 to 0.53419, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 85/250 [=========>....................] - ETA: 1:10:07 - loss: 1.2859 - accuracy: 0.5342\n",
            "Epoch 1: accuracy improved from 0.53419 to 0.53488, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 86/250 [=========>....................] - ETA: 1:09:41 - loss: 1.2822 - accuracy: 0.5349\n",
            "Epoch 1: accuracy improved from 0.53488 to 0.53664, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 87/250 [=========>....................] - ETA: 1:09:09 - loss: 1.2779 - accuracy: 0.5366\n",
            "Epoch 1: accuracy improved from 0.53664 to 0.53977, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 88/250 [=========>....................] - ETA: 1:08:37 - loss: 1.2708 - accuracy: 0.5398\n",
            "Epoch 1: accuracy improved from 0.53977 to 0.54249, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 89/250 [=========>....................] - ETA: 1:07:56 - loss: 1.2651 - accuracy: 0.5425\n",
            "Epoch 1: accuracy improved from 0.54249 to 0.54479, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 90/250 [=========>....................] - ETA: 1:07:30 - loss: 1.2614 - accuracy: 0.5448\n",
            "Epoch 1: accuracy improved from 0.54479 to 0.54670, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 91/250 [=========>....................] - ETA: 1:07:06 - loss: 1.2570 - accuracy: 0.5467\n",
            "Epoch 1: accuracy improved from 0.54670 to 0.54891, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 92/250 [==========>...................] - ETA: 1:06:45 - loss: 1.2510 - accuracy: 0.5489\n",
            "Epoch 1: accuracy improved from 0.54891 to 0.55040, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 93/250 [==========>...................] - ETA: 1:06:24 - loss: 1.2449 - accuracy: 0.5504\n",
            "Epoch 1: accuracy improved from 0.55040 to 0.55253, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 94/250 [==========>...................] - ETA: 1:05:58 - loss: 1.2386 - accuracy: 0.5525\n",
            "Epoch 1: accuracy improved from 0.55253 to 0.55461, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 95/250 [==========>...................] - ETA: 1:05:34 - loss: 1.2328 - accuracy: 0.5546\n",
            "Epoch 1: accuracy improved from 0.55461 to 0.55566, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 96/250 [==========>...................] - ETA: 1:05:10 - loss: 1.2271 - accuracy: 0.5557\n",
            "Epoch 1: accuracy improved from 0.55566 to 0.55606, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 97/250 [==========>...................] - ETA: 1:04:42 - loss: 1.2226 - accuracy: 0.5561\n",
            "Epoch 1: accuracy improved from 0.55606 to 0.55740, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 98/250 [==========>...................] - ETA: 1:04:21 - loss: 1.2186 - accuracy: 0.5574\n",
            "Epoch 1: accuracy improved from 0.55740 to 0.55966, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 99/250 [==========>...................] - ETA: 1:03:55 - loss: 1.2126 - accuracy: 0.5597\n",
            "Epoch 1: accuracy improved from 0.55966 to 0.56094, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "100/250 [===========>..................] - ETA: 1:03:35 - loss: 1.2078 - accuracy: 0.5609\n",
            "Epoch 1: accuracy improved from 0.56094 to 0.56250, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "101/250 [===========>..................] - ETA: 1:03:11 - loss: 1.2018 - accuracy: 0.5625\n",
            "Epoch 1: accuracy improved from 0.56250 to 0.56342, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "102/250 [===========>..................] - ETA: 1:02:46 - loss: 1.1972 - accuracy: 0.5634\n",
            "Epoch 1: accuracy improved from 0.56342 to 0.56462, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "103/250 [===========>..................] - ETA: 1:02:15 - loss: 1.1933 - accuracy: 0.5646\n",
            "Epoch 1: accuracy improved from 0.56462 to 0.56671, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "104/250 [===========>..................] - ETA: 1:01:51 - loss: 1.1896 - accuracy: 0.5667\n",
            "Epoch 1: accuracy improved from 0.56671 to 0.56964, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "105/250 [===========>..................] - ETA: 1:01:31 - loss: 1.1831 - accuracy: 0.5696\n",
            "Epoch 1: accuracy improved from 0.56964 to 0.57017, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "106/250 [===========>..................] - ETA: 1:01:04 - loss: 1.1795 - accuracy: 0.5702\n",
            "Epoch 1: accuracy improved from 0.57017 to 0.57097, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "107/250 [===========>..................] - ETA: 1:00:38 - loss: 1.1758 - accuracy: 0.5710\n",
            "Epoch 1: accuracy improved from 0.57097 to 0.57118, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "108/250 [===========>..................] - ETA: 1:00:14 - loss: 1.1740 - accuracy: 0.5712\n",
            "Epoch 1: accuracy improved from 0.57118 to 0.57483, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "109/250 [============>.................] - ETA: 59:49 - loss: 1.1661 - accuracy: 0.5748  \n",
            "Epoch 1: accuracy improved from 0.57483 to 0.57670, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "110/250 [============>.................] - ETA: 59:23 - loss: 1.1618 - accuracy: 0.5767\n",
            "Epoch 1: accuracy improved from 0.57670 to 0.57883, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "111/250 [============>.................] - ETA: 58:58 - loss: 1.1560 - accuracy: 0.5788\n",
            "Epoch 1: accuracy improved from 0.57883 to 0.57924, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "112/250 [============>.................] - ETA: 58:35 - loss: 1.1528 - accuracy: 0.5792\n",
            "Epoch 1: accuracy improved from 0.57924 to 0.58158, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "113/250 [============>.................] - ETA: 58:14 - loss: 1.1474 - accuracy: 0.5816\n",
            "Epoch 1: accuracy improved from 0.58158 to 0.58333, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "114/250 [============>.................] - ETA: 57:50 - loss: 1.1442 - accuracy: 0.5833\n",
            "Epoch 1: accuracy improved from 0.58333 to 0.58560, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "115/250 [============>.................] - ETA: 57:30 - loss: 1.1387 - accuracy: 0.5856\n",
            "Epoch 1: accuracy improved from 0.58560 to 0.58702, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "116/250 [============>.................] - ETA: 57:05 - loss: 1.1355 - accuracy: 0.5870\n",
            "Epoch 1: accuracy improved from 0.58702 to 0.59001, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "117/250 [=============>................] - ETA: 56:44 - loss: 1.1288 - accuracy: 0.5900\n",
            "Epoch 1: accuracy improved from 0.59001 to 0.59110, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "118/250 [=============>................] - ETA: 56:20 - loss: 1.1245 - accuracy: 0.5911\n",
            "Epoch 1: accuracy improved from 0.59110 to 0.59217, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "119/250 [=============>................] - ETA: 55:55 - loss: 1.1205 - accuracy: 0.5922\n",
            "Epoch 1: accuracy improved from 0.59217 to 0.59271, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "120/250 [=============>................] - ETA: 55:31 - loss: 1.1164 - accuracy: 0.5927\n",
            "Epoch 1: accuracy improved from 0.59271 to 0.59427, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "121/250 [=============>................] - ETA: 55:05 - loss: 1.1125 - accuracy: 0.5943\n",
            "Epoch 1: accuracy improved from 0.59427 to 0.59503, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "122/250 [=============>................] - ETA: 54:43 - loss: 1.1091 - accuracy: 0.5950\n",
            "Epoch 1: accuracy improved from 0.59503 to 0.59731, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "123/250 [=============>................] - ETA: 54:15 - loss: 1.1037 - accuracy: 0.5973\n",
            "Epoch 1: accuracy improved from 0.59731 to 0.59904, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "124/250 [=============>................] - ETA: 53:52 - loss: 1.0985 - accuracy: 0.5990\n",
            "Epoch 1: accuracy improved from 0.59904 to 0.59950, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "125/250 [==============>...............] - ETA: 53:24 - loss: 1.0969 - accuracy: 0.5995\n",
            "Epoch 1: accuracy improved from 0.59950 to 0.60045, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "126/250 [==============>...............] - ETA: 52:59 - loss: 1.0947 - accuracy: 0.6004\n",
            "Epoch 1: accuracy did not improve from 0.60045\n",
            "127/250 [==============>...............] - ETA: 52:29 - loss: 1.0925 - accuracy: 0.6001\n",
            "Epoch 1: accuracy improved from 0.60045 to 0.60181, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "128/250 [==============>...............] - ETA: 52:05 - loss: 1.0895 - accuracy: 0.6018\n",
            "Epoch 1: accuracy improved from 0.60181 to 0.60296, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "129/250 [==============>...............] - ETA: 51:39 - loss: 1.0864 - accuracy: 0.6030\n",
            "Epoch 1: accuracy improved from 0.60296 to 0.60409, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "130/250 [==============>...............] - ETA: 51:14 - loss: 1.0825 - accuracy: 0.6041\n",
            "Epoch 1: accuracy improved from 0.60409 to 0.60472, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "131/250 [==============>...............] - ETA: 50:48 - loss: 1.0806 - accuracy: 0.6047\n",
            "Epoch 1: accuracy improved from 0.60472 to 0.60724, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "132/250 [==============>...............] - ETA: 50:25 - loss: 1.0748 - accuracy: 0.6072\n",
            "Epoch 1: accuracy did not improve from 0.60724\n",
            "133/250 [==============>...............] - ETA: 49:56 - loss: 1.0734 - accuracy: 0.6071\n",
            "Epoch 1: accuracy improved from 0.60724 to 0.60914, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "134/250 [===============>..............] - ETA: 49:33 - loss: 1.0696 - accuracy: 0.6091\n",
            "Epoch 1: accuracy improved from 0.60914 to 0.61042, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "135/250 [===============>..............] - ETA: 49:05 - loss: 1.0659 - accuracy: 0.6104\n",
            "Epoch 1: accuracy improved from 0.61042 to 0.61167, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "136/250 [===============>..............] - ETA: 48:38 - loss: 1.0625 - accuracy: 0.6117\n",
            "Epoch 1: accuracy improved from 0.61167 to 0.61291, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "137/250 [===============>..............] - ETA: 48:11 - loss: 1.0587 - accuracy: 0.6129\n",
            "Epoch 1: accuracy improved from 0.61291 to 0.61345, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "138/250 [===============>..............] - ETA: 47:44 - loss: 1.0559 - accuracy: 0.6135\n",
            "Epoch 1: accuracy improved from 0.61345 to 0.61511, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "139/250 [===============>..............] - ETA: 47:19 - loss: 1.0526 - accuracy: 0.6151\n",
            "Epoch 1: accuracy improved from 0.61511 to 0.61652, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "140/250 [===============>..............] - ETA: 46:50 - loss: 1.0480 - accuracy: 0.6165\n",
            "Epoch 1: accuracy improved from 0.61652 to 0.61702, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "141/250 [===============>..............] - ETA: 46:25 - loss: 1.0467 - accuracy: 0.6170\n",
            "Epoch 1: accuracy improved from 0.61702 to 0.61752, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "142/250 [================>.............] - ETA: 45:56 - loss: 1.0441 - accuracy: 0.6175\n",
            "Epoch 1: accuracy improved from 0.61752 to 0.61801, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "143/250 [================>.............] - ETA: 45:31 - loss: 1.0405 - accuracy: 0.6180\n",
            "Epoch 1: accuracy improved from 0.61801 to 0.61892, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "144/250 [================>.............] - ETA: 45:02 - loss: 1.0380 - accuracy: 0.6189\n",
            "Epoch 1: accuracy improved from 0.61892 to 0.62004, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "145/250 [================>.............] - ETA: 44:37 - loss: 1.0344 - accuracy: 0.6200\n",
            "Epoch 1: accuracy improved from 0.62004 to 0.62051, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "146/250 [================>.............] - ETA: 44:12 - loss: 1.0316 - accuracy: 0.6205\n",
            "Epoch 1: accuracy improved from 0.62051 to 0.62096, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "147/250 [================>.............] - ETA: 43:49 - loss: 1.0304 - accuracy: 0.6210\n",
            "Epoch 1: accuracy improved from 0.62096 to 0.62120, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "148/250 [================>.............] - ETA: 43:21 - loss: 1.0301 - accuracy: 0.6212\n",
            "Epoch 1: accuracy improved from 0.62120 to 0.62227, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "149/250 [================>.............] - ETA: 42:56 - loss: 1.0260 - accuracy: 0.6223\n",
            "Epoch 1: accuracy improved from 0.62227 to 0.62313, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "150/250 [=================>............] - ETA: 42:30 - loss: 1.0233 - accuracy: 0.6231\n",
            "Epoch 1: accuracy improved from 0.62313 to 0.62355, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "151/250 [=================>............] - ETA: 42:04 - loss: 1.0228 - accuracy: 0.6236\n",
            "Epoch 1: accuracy improved from 0.62355 to 0.62397, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "152/250 [=================>............] - ETA: 41:39 - loss: 1.0217 - accuracy: 0.6240\n",
            "Epoch 1: accuracy improved from 0.62397 to 0.62480, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "153/250 [=================>............] - ETA: 41:13 - loss: 1.0190 - accuracy: 0.6248\n",
            "Epoch 1: accuracy improved from 0.62480 to 0.62642, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "154/250 [=================>............] - ETA: 40:48 - loss: 1.0154 - accuracy: 0.6264\n",
            "Epoch 1: accuracy improved from 0.62642 to 0.62823, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "155/250 [=================>............] - ETA: 40:22 - loss: 1.0124 - accuracy: 0.6282\n",
            "Epoch 1: accuracy improved from 0.62823 to 0.62941, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "156/250 [=================>............] - ETA: 39:57 - loss: 1.0091 - accuracy: 0.6294\n",
            "Epoch 1: accuracy improved from 0.62941 to 0.62998, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "157/250 [=================>............] - ETA: 39:32 - loss: 1.0068 - accuracy: 0.6300\n",
            "Epoch 1: accuracy improved from 0.62998 to 0.63113, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "158/250 [=================>............] - ETA: 39:05 - loss: 1.0039 - accuracy: 0.6311\n",
            "Epoch 1: accuracy improved from 0.63113 to 0.63168, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "159/250 [==================>...........] - ETA: 38:39 - loss: 1.0035 - accuracy: 0.6317\n",
            "Epoch 1: accuracy improved from 0.63168 to 0.63262, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "160/250 [==================>...........] - ETA: 38:14 - loss: 1.0016 - accuracy: 0.6326\n",
            "Epoch 1: accuracy improved from 0.63262 to 0.63412, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "161/250 [==================>...........] - ETA: 37:49 - loss: 0.9979 - accuracy: 0.6341\n",
            "Epoch 1: accuracy improved from 0.63412 to 0.63503, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "162/250 [==================>...........] - ETA: 37:22 - loss: 0.9956 - accuracy: 0.6350\n",
            "Epoch 1: accuracy improved from 0.63503 to 0.63612, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "163/250 [==================>...........] - ETA: 36:56 - loss: 0.9923 - accuracy: 0.6361\n",
            "Epoch 1: accuracy improved from 0.63612 to 0.63777, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "164/250 [==================>...........] - ETA: 36:28 - loss: 0.9878 - accuracy: 0.6378\n",
            "Epoch 1: accuracy improved from 0.63777 to 0.63864, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "165/250 [==================>...........] - ETA: 36:03 - loss: 0.9859 - accuracy: 0.6386\n",
            "Epoch 1: accuracy improved from 0.63864 to 0.63893, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "166/250 [==================>...........] - ETA: 35:35 - loss: 0.9855 - accuracy: 0.6389\n",
            "Epoch 1: accuracy improved from 0.63893 to 0.63997, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "167/250 [===================>..........] - ETA: 35:06 - loss: 0.9829 - accuracy: 0.6400\n",
            "Epoch 1: accuracy improved from 0.63997 to 0.64081, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "168/250 [===================>..........] - ETA: 34:41 - loss: 0.9806 - accuracy: 0.6408\n",
            "Epoch 1: accuracy improved from 0.64081 to 0.64164, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "169/250 [===================>..........] - ETA: 34:16 - loss: 0.9787 - accuracy: 0.6416\n",
            "Epoch 1: accuracy improved from 0.64164 to 0.64228, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "170/250 [===================>..........] - ETA: 33:51 - loss: 0.9774 - accuracy: 0.6423\n",
            "Epoch 1: accuracy improved from 0.64228 to 0.64309, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "171/250 [===================>..........] - ETA: 33:26 - loss: 0.9748 - accuracy: 0.6431\n",
            "Epoch 1: accuracy improved from 0.64309 to 0.64408, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "172/250 [===================>..........] - ETA: 32:49 - loss: 0.9721 - accuracy: 0.6441\n",
            "Epoch 1: accuracy improved from 0.64408 to 0.64487, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "173/250 [===================>..........] - ETA: 32:27 - loss: 0.9699 - accuracy: 0.6449\n",
            "Epoch 1: accuracy improved from 0.64487 to 0.64511, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "174/250 [===================>..........] - ETA: 32:02 - loss: 0.9691 - accuracy: 0.6451\n",
            "Epoch 1: accuracy improved from 0.64511 to 0.64589, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "175/250 [====================>.........] - ETA: 31:36 - loss: 0.9673 - accuracy: 0.6459\n",
            "Epoch 1: accuracy improved from 0.64589 to 0.64648, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "176/250 [====================>.........] - ETA: 31:10 - loss: 0.9653 - accuracy: 0.6465\n",
            "Epoch 1: accuracy improved from 0.64648 to 0.64778, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "177/250 [====================>.........] - ETA: 30:44 - loss: 0.9620 - accuracy: 0.6478\n",
            "Epoch 1: accuracy improved from 0.64778 to 0.64853, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "178/250 [====================>.........] - ETA: 30:18 - loss: 0.9596 - accuracy: 0.6485\n",
            "Epoch 1: accuracy improved from 0.64853 to 0.64962, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "179/250 [====================>.........] - ETA: 29:53 - loss: 0.9567 - accuracy: 0.6496\n",
            "Epoch 1: accuracy improved from 0.64962 to 0.65052, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "180/250 [====================>.........] - ETA: 29:26 - loss: 0.9546 - accuracy: 0.6505\n",
            "Epoch 1: accuracy improved from 0.65052 to 0.65159, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "181/250 [====================>.........] - ETA: 29:01 - loss: 0.9526 - accuracy: 0.6516\n",
            "Epoch 1: accuracy improved from 0.65159 to 0.65264, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "182/250 [====================>.........] - ETA: 28:36 - loss: 0.9509 - accuracy: 0.6526\n",
            "Epoch 1: accuracy improved from 0.65264 to 0.65283, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "183/250 [====================>.........] - ETA: 28:12 - loss: 0.9493 - accuracy: 0.6528\n",
            "Epoch 1: accuracy improved from 0.65283 to 0.65387, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "184/250 [=====================>........] - ETA: 27:46 - loss: 0.9464 - accuracy: 0.6539\n",
            "Epoch 1: accuracy improved from 0.65387 to 0.65490, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "185/250 [=====================>........] - ETA: 27:20 - loss: 0.9447 - accuracy: 0.6549\n",
            "Epoch 1: accuracy improved from 0.65490 to 0.65591, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "186/250 [=====================>........] - ETA: 26:56 - loss: 0.9418 - accuracy: 0.6559\n",
            "Epoch 1: accuracy improved from 0.65591 to 0.65658, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "187/250 [=====================>........] - ETA: 26:32 - loss: 0.9406 - accuracy: 0.6566\n",
            "Epoch 1: accuracy improved from 0.65658 to 0.65708, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "188/250 [=====================>........] - ETA: 26:06 - loss: 0.9385 - accuracy: 0.6571\n",
            "Epoch 1: accuracy improved from 0.65708 to 0.65741, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "189/250 [=====================>........] - ETA: 25:40 - loss: 0.9375 - accuracy: 0.6574\n",
            "Epoch 1: accuracy improved from 0.65741 to 0.65806, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "190/250 [=====================>........] - ETA: 25:14 - loss: 0.9357 - accuracy: 0.6581\n",
            "Epoch 1: accuracy improved from 0.65806 to 0.65854, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "191/250 [=====================>........] - ETA: 24:48 - loss: 0.9338 - accuracy: 0.6585\n",
            "Epoch 1: accuracy improved from 0.65854 to 0.65999, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "192/250 [======================>.......] - ETA: 24:24 - loss: 0.9303 - accuracy: 0.6600\n",
            "Epoch 1: accuracy improved from 0.65999 to 0.66014, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "193/250 [======================>.......] - ETA: 23:59 - loss: 0.9294 - accuracy: 0.6601\n",
            "Epoch 1: accuracy improved from 0.66014 to 0.66076, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "194/250 [======================>.......] - ETA: 23:34 - loss: 0.9271 - accuracy: 0.6608\n",
            "Epoch 1: accuracy improved from 0.66076 to 0.66154, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "195/250 [======================>.......] - ETA: 23:08 - loss: 0.9247 - accuracy: 0.6615\n",
            "Epoch 1: accuracy improved from 0.66154 to 0.66247, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "196/250 [======================>.......] - ETA: 22:43 - loss: 0.9227 - accuracy: 0.6625\n",
            "Epoch 1: accuracy improved from 0.66247 to 0.66323, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "197/250 [======================>.......] - ETA: 22:17 - loss: 0.9204 - accuracy: 0.6632\n",
            "Epoch 1: accuracy improved from 0.66323 to 0.66367, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "198/250 [======================>.......] - ETA: 21:52 - loss: 0.9185 - accuracy: 0.6637\n",
            "Epoch 1: accuracy improved from 0.66367 to 0.66442, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "199/250 [======================>.......] - ETA: 21:27 - loss: 0.9164 - accuracy: 0.6644\n",
            "Epoch 1: accuracy improved from 0.66442 to 0.66489, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "200/250 [=======================>......] - ETA: 20:58 - loss: 0.9156 - accuracy: 0.6649\n",
            "Epoch 1: accuracy improved from 0.66489 to 0.66594, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "201/250 [=======================>......] - ETA: 20:33 - loss: 0.9135 - accuracy: 0.6659\n",
            "Epoch 1: accuracy improved from 0.66594 to 0.66636, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "202/250 [=======================>......] - ETA: 20:08 - loss: 0.9123 - accuracy: 0.6664\n",
            "Epoch 1: accuracy improved from 0.66636 to 0.66692, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "203/250 [=======================>......] - ETA: 19:42 - loss: 0.9107 - accuracy: 0.6669\n",
            "Epoch 1: accuracy improved from 0.66692 to 0.66764, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "204/250 [=======================>......] - ETA: 19:18 - loss: 0.9084 - accuracy: 0.6676\n",
            "Epoch 1: accuracy improved from 0.66764 to 0.66881, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "205/250 [=======================>......] - ETA: 18:53 - loss: 0.9056 - accuracy: 0.6688\n",
            "Epoch 1: accuracy improved from 0.66881 to 0.66996, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "206/250 [=======================>......] - ETA: 18:28 - loss: 0.9029 - accuracy: 0.6700\n",
            "Epoch 1: accuracy improved from 0.66996 to 0.67081, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "207/250 [=======================>......] - ETA: 18:03 - loss: 0.9024 - accuracy: 0.6708\n",
            "Epoch 1: accuracy improved from 0.67081 to 0.67194, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "208/250 [=======================>......] - ETA: 17:38 - loss: 0.8997 - accuracy: 0.6719\n",
            "Epoch 1: accuracy improved from 0.67194 to 0.67232, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "209/250 [========================>.....] - ETA: 17:14 - loss: 0.8981 - accuracy: 0.6723\n",
            "Epoch 1: accuracy improved from 0.67232 to 0.67313, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "210/250 [========================>.....] - ETA: 16:48 - loss: 0.8959 - accuracy: 0.6731\n",
            "Epoch 1: accuracy improved from 0.67313 to 0.67365, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "211/250 [========================>.....] - ETA: 16:23 - loss: 0.8936 - accuracy: 0.6736\n",
            "Epoch 1: accuracy improved from 0.67365 to 0.67386, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "212/250 [========================>.....] - ETA: 15:57 - loss: 0.8927 - accuracy: 0.6739\n",
            "Epoch 1: accuracy improved from 0.67386 to 0.67466, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "213/250 [========================>.....] - ETA: 15:32 - loss: 0.8908 - accuracy: 0.6747\n",
            "Epoch 1: accuracy improved from 0.67466 to 0.67545, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "214/250 [========================>.....] - ETA: 15:06 - loss: 0.8890 - accuracy: 0.6755\n",
            "Epoch 1: accuracy improved from 0.67545 to 0.67595, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "215/250 [========================>.....] - ETA: 14:41 - loss: 0.8876 - accuracy: 0.6759\n",
            "Epoch 1: accuracy improved from 0.67595 to 0.67658, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "216/250 [========================>.....] - ETA: 14:17 - loss: 0.8855 - accuracy: 0.6766\n",
            "Epoch 1: accuracy improved from 0.67658 to 0.67707, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "217/250 [=========================>....] - ETA: 13:51 - loss: 0.8835 - accuracy: 0.6771\n",
            "Epoch 1: accuracy improved from 0.67707 to 0.67769, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "218/250 [=========================>....] - ETA: 13:26 - loss: 0.8812 - accuracy: 0.6777\n",
            "Epoch 1: accuracy improved from 0.67769 to 0.67816, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "219/250 [=========================>....] - ETA: 13:00 - loss: 0.8795 - accuracy: 0.6782\n",
            "Epoch 1: accuracy improved from 0.67816 to 0.67849, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "220/250 [=========================>....] - ETA: 12:35 - loss: 0.8780 - accuracy: 0.6785\n",
            "Epoch 1: accuracy improved from 0.67849 to 0.67881, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "221/250 [=========================>....] - ETA: 12:09 - loss: 0.8771 - accuracy: 0.6788\n",
            "Epoch 1: accuracy improved from 0.67881 to 0.67899, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "222/250 [=========================>....] - ETA: 11:45 - loss: 0.8757 - accuracy: 0.6790\n",
            "Epoch 1: accuracy improved from 0.67899 to 0.68016, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "223/250 [=========================>....] - ETA: 11:19 - loss: 0.8731 - accuracy: 0.6802\n",
            "Epoch 1: accuracy improved from 0.68016 to 0.68103, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "224/250 [=========================>....] - ETA: 10:54 - loss: 0.8712 - accuracy: 0.6810\n",
            "Epoch 1: accuracy improved from 0.68103 to 0.68175, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "225/250 [==========================>...] - ETA: 10:29 - loss: 0.8694 - accuracy: 0.6818\n",
            "Epoch 1: accuracy improved from 0.68175 to 0.68261, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "226/250 [==========================>...] - ETA: 10:04 - loss: 0.8671 - accuracy: 0.6826\n",
            "Epoch 1: accuracy improved from 0.68261 to 0.68291, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "227/250 [==========================>...] - ETA: 9:38 - loss: 0.8664 - accuracy: 0.6829 \n",
            "Epoch 1: accuracy improved from 0.68291 to 0.68307, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "228/250 [==========================>...] - ETA: 9:13 - loss: 0.8654 - accuracy: 0.6831\n",
            "Epoch 1: accuracy improved from 0.68307 to 0.68363, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "229/250 [==========================>...] - ETA: 8:48 - loss: 0.8637 - accuracy: 0.6836\n",
            "Epoch 1: accuracy improved from 0.68363 to 0.68420, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "230/250 [==========================>...] - ETA: 8:23 - loss: 0.8619 - accuracy: 0.6842\n",
            "Epoch 1: accuracy improved from 0.68420 to 0.68530, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "231/250 [==========================>...] - ETA: 7:58 - loss: 0.8592 - accuracy: 0.6853\n",
            "Epoch 1: accuracy improved from 0.68530 to 0.68531, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "232/250 [==========================>...] - ETA: 7:33 - loss: 0.8589 - accuracy: 0.6853\n",
            "Epoch 1: accuracy improved from 0.68531 to 0.68612, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "233/250 [==========================>...] - ETA: 7:07 - loss: 0.8568 - accuracy: 0.6861\n",
            "Epoch 1: accuracy improved from 0.68612 to 0.68706, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "234/250 [===========================>..] - ETA: 6:42 - loss: 0.8548 - accuracy: 0.6871\n",
            "Epoch 1: accuracy improved from 0.68706 to 0.68787, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "235/250 [===========================>..] - ETA: 6:17 - loss: 0.8534 - accuracy: 0.6879\n",
            "Epoch 1: accuracy did not improve from 0.68787\n",
            "236/250 [===========================>..] - ETA: 5:52 - loss: 0.8522 - accuracy: 0.6879\n",
            "Epoch 1: accuracy improved from 0.68787 to 0.68839, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "237/250 [===========================>..] - ETA: 5:27 - loss: 0.8507 - accuracy: 0.6884\n",
            "Epoch 1: accuracy improved from 0.68839 to 0.68905, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "238/250 [===========================>..] - ETA: 5:02 - loss: 0.8491 - accuracy: 0.6890\n",
            "Epoch 1: accuracy improved from 0.68905 to 0.68970, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "239/250 [===========================>..] - ETA: 4:37 - loss: 0.8472 - accuracy: 0.6897\n",
            "Epoch 1: accuracy improved from 0.68970 to 0.69008, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "240/250 [===========================>..] - ETA: 4:11 - loss: 0.8460 - accuracy: 0.6901\n",
            "Epoch 1: accuracy improved from 0.69008 to 0.69059, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "241/250 [===========================>..] - ETA: 3:46 - loss: 0.8453 - accuracy: 0.6906\n",
            "Epoch 1: accuracy improved from 0.69059 to 0.69122, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "242/250 [============================>.] - ETA: 3:21 - loss: 0.8442 - accuracy: 0.6912\n",
            "Epoch 1: accuracy improved from 0.69122 to 0.69159, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "243/250 [============================>.] - ETA: 2:56 - loss: 0.8430 - accuracy: 0.6916\n",
            "Epoch 1: accuracy improved from 0.69159 to 0.69248, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "244/250 [============================>.] - ETA: 2:31 - loss: 0.8407 - accuracy: 0.6925\n",
            "Epoch 1: accuracy improved from 0.69248 to 0.69297, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "245/250 [============================>.] - ETA: 2:05 - loss: 0.8394 - accuracy: 0.6930\n",
            "Epoch 1: accuracy improved from 0.69297 to 0.69345, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "246/250 [============================>.] - ETA: 1:40 - loss: 0.8381 - accuracy: 0.6935\n",
            "Epoch 1: accuracy improved from 0.69345 to 0.69406, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "247/250 [============================>.] - ETA: 1:15 - loss: 0.8363 - accuracy: 0.6941\n",
            "Epoch 1: accuracy improved from 0.69406 to 0.69454, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "248/250 [============================>.] - ETA: 50s - loss: 0.8350 - accuracy: 0.6945 \n",
            "Epoch 1: accuracy improved from 0.69454 to 0.69527, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "249/250 [============================>.] - ETA: 25s - loss: 0.8332 - accuracy: 0.6953\n",
            "Epoch 1: accuracy improved from 0.69527 to 0.69549, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "250/250 [==============================] - 7792s 31s/step - loss: 0.8325 - accuracy: 0.6955 - val_loss: 0.3233 - val_accuracy: 0.9052\n",
            "Epoch 2/5\n",
            "\n",
            "Epoch 2: accuracy improved from 0.69549 to 0.78125, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  1/250 [..............................] - ETA: 19:50 - loss: 0.6358 - accuracy: 0.7812\n",
            "Epoch 2: accuracy did not improve from 0.78125\n",
            "  2/250 [..............................] - ETA: 1:00 - loss: 0.6958 - accuracy: 0.7812 \n",
            "Epoch 2: accuracy improved from 0.78125 to 0.79167, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  3/250 [..............................] - ETA: 5:14 - loss: 0.6216 - accuracy: 0.7917\n",
            "Epoch 2: accuracy improved from 0.79167 to 0.81250, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  4/250 [..............................] - ETA: 6:46 - loss: 0.5755 - accuracy: 0.8125\n",
            "Epoch 2: accuracy did not improve from 0.81250\n",
            "  5/250 [..............................] - ETA: 5:17 - loss: 0.5649 - accuracy: 0.8000\n",
            "Epoch 2: accuracy improved from 0.81250 to 0.82812, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  6/250 [..............................] - ETA: 6:05 - loss: 0.5047 - accuracy: 0.8281\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            "  7/250 [..............................] - ETA: 5:10 - loss: 0.5030 - accuracy: 0.8170\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            "  8/250 [..............................] - ETA: 4:40 - loss: 0.5247 - accuracy: 0.7969\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            "  9/250 [>.............................] - ETA: 4:18 - loss: 0.5295 - accuracy: 0.7986\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 10/250 [>.............................] - ETA: 4:06 - loss: 0.5075 - accuracy: 0.8094\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 11/250 [>.............................] - ETA: 3:58 - loss: 0.5199 - accuracy: 0.8040\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 12/250 [>.............................] - ETA: 3:50 - loss: 0.5192 - accuracy: 0.8073\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 13/250 [>.............................] - ETA: 3:44 - loss: 0.5014 - accuracy: 0.8173\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 14/250 [>.............................] - ETA: 3:38 - loss: 0.5001 - accuracy: 0.8103\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 15/250 [>.............................] - ETA: 3:30 - loss: 0.5008 - accuracy: 0.8042\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 16/250 [>.............................] - ETA: 3:22 - loss: 0.5084 - accuracy: 0.7988\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 17/250 [=>............................] - ETA: 3:15 - loss: 0.4994 - accuracy: 0.8015\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 18/250 [=>............................] - ETA: 3:10 - loss: 0.5049 - accuracy: 0.8021\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 19/250 [=>............................] - ETA: 3:05 - loss: 0.5175 - accuracy: 0.7993\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 20/250 [=>............................] - ETA: 3:01 - loss: 0.5098 - accuracy: 0.8047\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 21/250 [=>............................] - ETA: 2:58 - loss: 0.5054 - accuracy: 0.8065\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 22/250 [=>............................] - ETA: 2:54 - loss: 0.5004 - accuracy: 0.8097\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 23/250 [=>............................] - ETA: 2:51 - loss: 0.5084 - accuracy: 0.8057\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 24/250 [=>............................] - ETA: 2:48 - loss: 0.5098 - accuracy: 0.8060\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 25/250 [==>...........................] - ETA: 2:44 - loss: 0.5073 - accuracy: 0.8087\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 26/250 [==>...........................] - ETA: 2:41 - loss: 0.5134 - accuracy: 0.8089\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 27/250 [==>...........................] - ETA: 2:38 - loss: 0.5108 - accuracy: 0.8102\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 28/250 [==>...........................] - ETA: 2:35 - loss: 0.5055 - accuracy: 0.8125\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 29/250 [==>...........................] - ETA: 2:33 - loss: 0.5109 - accuracy: 0.8114\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 30/250 [==>...........................] - ETA: 2:30 - loss: 0.5085 - accuracy: 0.8146\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 31/250 [==>...........................] - ETA: 2:28 - loss: 0.5130 - accuracy: 0.8125\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 32/250 [==>...........................] - ETA: 2:26 - loss: 0.5076 - accuracy: 0.8164\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 33/250 [==>...........................] - ETA: 2:24 - loss: 0.5051 - accuracy: 0.8182\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 34/250 [===>..........................] - ETA: 2:22 - loss: 0.5074 - accuracy: 0.8199\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 35/250 [===>..........................] - ETA: 2:20 - loss: 0.5026 - accuracy: 0.8214\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 36/250 [===>..........................] - ETA: 2:19 - loss: 0.5078 - accuracy: 0.8194\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 37/250 [===>..........................] - ETA: 2:19 - loss: 0.5067 - accuracy: 0.8193\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 38/250 [===>..........................] - ETA: 2:18 - loss: 0.5016 - accuracy: 0.8207\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 39/250 [===>..........................] - ETA: 2:18 - loss: 0.4986 - accuracy: 0.8229\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 40/250 [===>..........................] - ETA: 2:17 - loss: 0.4990 - accuracy: 0.8234\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 41/250 [===>..........................] - ETA: 2:16 - loss: 0.5003 - accuracy: 0.8232\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 42/250 [====>.........................] - ETA: 2:14 - loss: 0.5013 - accuracy: 0.8229\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 43/250 [====>.........................] - ETA: 2:13 - loss: 0.5021 - accuracy: 0.8227\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 44/250 [====>.........................] - ETA: 2:11 - loss: 0.5080 - accuracy: 0.8210\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 45/250 [====>.........................] - ETA: 2:10 - loss: 0.5034 - accuracy: 0.8229\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 46/250 [====>.........................] - ETA: 2:08 - loss: 0.4985 - accuracy: 0.8247\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 47/250 [====>.........................] - ETA: 2:07 - loss: 0.4975 - accuracy: 0.8238\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 48/250 [====>.........................] - ETA: 2:05 - loss: 0.4929 - accuracy: 0.8255\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 49/250 [====>.........................] - ETA: 2:04 - loss: 0.4878 - accuracy: 0.8278\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 50/250 [=====>........................] - ETA: 2:03 - loss: 0.4860 - accuracy: 0.8269\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 51/250 [=====>........................] - ETA: 2:02 - loss: 0.4870 - accuracy: 0.8248\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 52/250 [=====>........................] - ETA: 2:01 - loss: 0.4892 - accuracy: 0.8245\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 53/250 [=====>........................] - ETA: 1:59 - loss: 0.4881 - accuracy: 0.8243\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 54/250 [=====>........................] - ETA: 1:58 - loss: 0.4873 - accuracy: 0.8252\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 55/250 [=====>........................] - ETA: 1:57 - loss: 0.4887 - accuracy: 0.8244\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 56/250 [=====>........................] - ETA: 1:56 - loss: 0.4905 - accuracy: 0.8242\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 57/250 [=====>........................] - ETA: 1:55 - loss: 0.4900 - accuracy: 0.8240\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 58/250 [=====>........................] - ETA: 1:54 - loss: 0.4881 - accuracy: 0.8249\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 59/250 [======>.......................] - ETA: 1:53 - loss: 0.4861 - accuracy: 0.8263\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 60/250 [======>.......................] - ETA: 1:52 - loss: 0.4874 - accuracy: 0.8250\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 61/250 [======>.......................] - ETA: 1:51 - loss: 0.4876 - accuracy: 0.8243\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 62/250 [======>.......................] - ETA: 1:50 - loss: 0.4854 - accuracy: 0.8256\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 63/250 [======>.......................] - ETA: 1:50 - loss: 0.4846 - accuracy: 0.8259\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 64/250 [======>.......................] - ETA: 1:50 - loss: 0.4852 - accuracy: 0.8257\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 65/250 [======>.......................] - ETA: 1:49 - loss: 0.4819 - accuracy: 0.8269\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 66/250 [======>.......................] - ETA: 1:49 - loss: 0.4810 - accuracy: 0.8272\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 67/250 [=======>......................] - ETA: 1:48 - loss: 0.4825 - accuracy: 0.8265\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 68/250 [=======>......................] - ETA: 1:47 - loss: 0.4799 - accuracy: 0.8263\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 69/250 [=======>......................] - ETA: 1:46 - loss: 0.4796 - accuracy: 0.8261\n",
            "Epoch 2: accuracy did not improve from 0.82812\n",
            " 70/250 [=======>......................] - ETA: 1:45 - loss: 0.4777 - accuracy: 0.8272\n",
            "Epoch 2: accuracy improved from 0.82812 to 0.82923, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 71/250 [=======>......................] - ETA: 1:53 - loss: 0.4744 - accuracy: 0.8292\n",
            "Epoch 2: accuracy improved from 0.82923 to 0.83030, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 72/250 [=======>......................] - ETA: 1:55 - loss: 0.4723 - accuracy: 0.8303\n",
            "Epoch 2: accuracy did not improve from 0.83030\n",
            " 73/250 [=======>......................] - ETA: 1:53 - loss: 0.4722 - accuracy: 0.8301\n",
            "Epoch 2: accuracy improved from 0.83030 to 0.83066, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 74/250 [=======>......................] - ETA: 2:01 - loss: 0.4702 - accuracy: 0.8307\n",
            "Epoch 2: accuracy did not improve from 0.83066\n",
            " 75/250 [========>.....................] - ETA: 1:59 - loss: 0.4687 - accuracy: 0.8304\n",
            "Epoch 2: accuracy did not improve from 0.83066\n",
            " 76/250 [========>.....................] - ETA: 1:59 - loss: 0.4692 - accuracy: 0.8302\n",
            "Epoch 2: accuracy improved from 0.83066 to 0.83117, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 77/250 [========>.....................] - ETA: 2:01 - loss: 0.4673 - accuracy: 0.8312\n",
            "Epoch 2: accuracy improved from 0.83117 to 0.83133, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 78/250 [========>.....................] - ETA: 2:02 - loss: 0.4672 - accuracy: 0.8313\n",
            "Epoch 2: accuracy improved from 0.83133 to 0.83214, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 79/250 [========>.....................] - ETA: 2:09 - loss: 0.4658 - accuracy: 0.8321\n",
            "Epoch 2: accuracy improved from 0.83214 to 0.83307, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 80/250 [========>.....................] - ETA: 2:10 - loss: 0.4652 - accuracy: 0.8331\n",
            "Epoch 2: accuracy did not improve from 0.83307\n",
            " 81/250 [========>.....................] - ETA: 2:08 - loss: 0.4668 - accuracy: 0.8328\n",
            "Epoch 2: accuracy did not improve from 0.83307\n",
            " 82/250 [========>.....................] - ETA: 2:07 - loss: 0.4673 - accuracy: 0.8329\n",
            "Epoch 2: accuracy did not improve from 0.83307\n",
            " 83/250 [========>.....................] - ETA: 2:06 - loss: 0.4672 - accuracy: 0.8327\n",
            "Epoch 2: accuracy did not improve from 0.83307\n",
            " 84/250 [=========>....................] - ETA: 2:06 - loss: 0.4686 - accuracy: 0.8321\n",
            "Epoch 2: accuracy did not improve from 0.83307\n",
            " 85/250 [=========>....................] - ETA: 2:05 - loss: 0.4681 - accuracy: 0.8322\n",
            "Epoch 2: accuracy improved from 0.83307 to 0.83309, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 86/250 [=========>....................] - ETA: 2:07 - loss: 0.4660 - accuracy: 0.8331\n",
            "Epoch 2: accuracy improved from 0.83309 to 0.83394, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            " 87/250 [=========>....................] - ETA: 2:08 - loss: 0.4653 - accuracy: 0.8339\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 88/250 [=========>....................] - ETA: 2:06 - loss: 0.4679 - accuracy: 0.8333\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 89/250 [=========>....................] - ETA: 2:05 - loss: 0.4711 - accuracy: 0.8327\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 90/250 [=========>....................] - ETA: 2:03 - loss: 0.4727 - accuracy: 0.8322\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 91/250 [=========>....................] - ETA: 2:02 - loss: 0.4711 - accuracy: 0.8330\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 92/250 [==========>...................] - ETA: 2:01 - loss: 0.4729 - accuracy: 0.8321\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 93/250 [==========>...................] - ETA: 1:59 - loss: 0.4734 - accuracy: 0.8319\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 94/250 [==========>...................] - ETA: 1:58 - loss: 0.4711 - accuracy: 0.8327\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 95/250 [==========>...................] - ETA: 1:57 - loss: 0.4713 - accuracy: 0.8328\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 96/250 [==========>...................] - ETA: 1:56 - loss: 0.4705 - accuracy: 0.8332\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 97/250 [==========>...................] - ETA: 1:54 - loss: 0.4704 - accuracy: 0.8324\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 98/250 [==========>...................] - ETA: 1:53 - loss: 0.4696 - accuracy: 0.8328\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            " 99/250 [==========>...................] - ETA: 1:52 - loss: 0.4683 - accuracy: 0.8332\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            "100/250 [===========>..................] - ETA: 1:51 - loss: 0.4688 - accuracy: 0.8333\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            "101/250 [===========>..................] - ETA: 1:50 - loss: 0.4694 - accuracy: 0.8328\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            "102/250 [===========>..................] - ETA: 1:49 - loss: 0.4693 - accuracy: 0.8326\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            "103/250 [===========>..................] - ETA: 1:48 - loss: 0.4685 - accuracy: 0.8330\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            "104/250 [===========>..................] - ETA: 1:47 - loss: 0.4689 - accuracy: 0.8328\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            "105/250 [===========>..................] - ETA: 1:47 - loss: 0.4675 - accuracy: 0.8332\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            "106/250 [===========>..................] - ETA: 1:46 - loss: 0.4657 - accuracy: 0.8339\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            "107/250 [===========>..................] - ETA: 1:45 - loss: 0.4661 - accuracy: 0.8334\n",
            "Epoch 2: accuracy did not improve from 0.83394\n",
            "108/250 [===========>..................] - ETA: 1:44 - loss: 0.4652 - accuracy: 0.8332\n",
            "Epoch 2: accuracy improved from 0.83394 to 0.83420, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "109/250 [============>.................] - ETA: 1:45 - loss: 0.4641 - accuracy: 0.8342\n",
            "Epoch 2: accuracy did not improve from 0.83420\n",
            "110/250 [============>.................] - ETA: 1:44 - loss: 0.4661 - accuracy: 0.8337\n",
            "Epoch 2: accuracy did not improve from 0.83420\n",
            "111/250 [============>.................] - ETA: 1:42 - loss: 0.4665 - accuracy: 0.8338\n",
            "Epoch 2: accuracy did not improve from 0.83420\n",
            "112/250 [============>.................] - ETA: 1:41 - loss: 0.4667 - accuracy: 0.8336\n",
            "Epoch 2: accuracy improved from 0.83420 to 0.83454, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "113/250 [============>.................] - ETA: 1:42 - loss: 0.4652 - accuracy: 0.8345\n",
            "Epoch 2: accuracy improved from 0.83454 to 0.83517, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "114/250 [============>.................] - ETA: 1:43 - loss: 0.4640 - accuracy: 0.8352\n",
            "Epoch 2: accuracy did not improve from 0.83517\n",
            "115/250 [============>.................] - ETA: 1:42 - loss: 0.4648 - accuracy: 0.8347\n",
            "Epoch 2: accuracy did not improve from 0.83517\n",
            "116/250 [============>.................] - ETA: 1:41 - loss: 0.4653 - accuracy: 0.8342\n",
            "Epoch 2: accuracy did not improve from 0.83517\n",
            "117/250 [=============>................] - ETA: 1:39 - loss: 0.4670 - accuracy: 0.8338\n",
            "Epoch 2: accuracy did not improve from 0.83517\n",
            "118/250 [=============>................] - ETA: 1:38 - loss: 0.4647 - accuracy: 0.8349\n",
            "Epoch 2: accuracy improved from 0.83517 to 0.83553, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "119/250 [=============>................] - ETA: 1:40 - loss: 0.4633 - accuracy: 0.8355\n",
            "Epoch 2: accuracy improved from 0.83553 to 0.83613, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "120/250 [=============>................] - ETA: 1:42 - loss: 0.4617 - accuracy: 0.8361\n",
            "Epoch 2: accuracy improved from 0.83613 to 0.83645, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "121/250 [=============>................] - ETA: 1:43 - loss: 0.4611 - accuracy: 0.8364\n",
            "Epoch 2: accuracy improved from 0.83645 to 0.83702, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "122/250 [=============>................] - ETA: 1:44 - loss: 0.4609 - accuracy: 0.8370\n",
            "Epoch 2: accuracy improved from 0.83702 to 0.83733, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "123/250 [=============>................] - ETA: 1:45 - loss: 0.4606 - accuracy: 0.8373\n",
            "Epoch 2: accuracy improved from 0.83733 to 0.83789, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "124/250 [=============>................] - ETA: 1:46 - loss: 0.4601 - accuracy: 0.8379\n",
            "Epoch 2: accuracy improved from 0.83789 to 0.83819, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "125/250 [==============>...............] - ETA: 1:46 - loss: 0.4588 - accuracy: 0.8382\n",
            "Epoch 2: accuracy improved from 0.83819 to 0.83824, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "126/250 [==============>...............] - ETA: 1:48 - loss: 0.4579 - accuracy: 0.8382\n",
            "Epoch 2: accuracy improved from 0.83824 to 0.83927, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "127/250 [==============>...............] - ETA: 1:48 - loss: 0.4566 - accuracy: 0.8393\n",
            "Epoch 2: accuracy improved from 0.83927 to 0.83930, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "128/250 [==============>...............] - ETA: 1:49 - loss: 0.4585 - accuracy: 0.8393\n",
            "Epoch 2: accuracy did not improve from 0.83930\n",
            "129/250 [==============>...............] - ETA: 1:48 - loss: 0.4600 - accuracy: 0.8386\n",
            "Epoch 2: accuracy did not improve from 0.83930\n",
            "130/250 [==============>...............] - ETA: 1:47 - loss: 0.4595 - accuracy: 0.8386\n",
            "Epoch 2: accuracy did not improve from 0.83930\n",
            "131/250 [==============>...............] - ETA: 1:46 - loss: 0.4593 - accuracy: 0.8384\n",
            "Epoch 2: accuracy did not improve from 0.83930\n",
            "132/250 [==============>...............] - ETA: 1:45 - loss: 0.4590 - accuracy: 0.8387\n",
            "Epoch 2: accuracy did not improve from 0.83930\n",
            "133/250 [==============>...............] - ETA: 1:44 - loss: 0.4592 - accuracy: 0.8385\n",
            "Epoch 2: accuracy did not improve from 0.83930\n",
            "134/250 [===============>..............] - ETA: 1:42 - loss: 0.4578 - accuracy: 0.8393\n",
            "Epoch 2: accuracy improved from 0.83930 to 0.84023, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "135/250 [===============>..............] - ETA: 1:42 - loss: 0.4561 - accuracy: 0.8402\n",
            "Epoch 2: accuracy improved from 0.84023 to 0.84049, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "136/250 [===============>..............] - ETA: 1:44 - loss: 0.4560 - accuracy: 0.8405\n",
            "Epoch 2: accuracy improved from 0.84049 to 0.84097, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "137/250 [===============>..............] - ETA: 1:44 - loss: 0.4542 - accuracy: 0.8410\n",
            "Epoch 2: accuracy improved from 0.84097 to 0.84122, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "138/250 [===============>..............] - ETA: 1:45 - loss: 0.4538 - accuracy: 0.8412\n",
            "Epoch 2: accuracy did not improve from 0.84122\n",
            "139/250 [===============>..............] - ETA: 1:43 - loss: 0.4541 - accuracy: 0.8410\n",
            "Epoch 2: accuracy improved from 0.84122 to 0.84126, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "140/250 [===============>..............] - ETA: 1:44 - loss: 0.4535 - accuracy: 0.8413\n",
            "Epoch 2: accuracy improved from 0.84126 to 0.84172, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "141/250 [===============>..............] - ETA: 1:44 - loss: 0.4529 - accuracy: 0.8417\n",
            "Epoch 2: accuracy did not improve from 0.84172\n",
            "142/250 [================>.............] - ETA: 1:42 - loss: 0.4537 - accuracy: 0.8411\n",
            "Epoch 2: accuracy improved from 0.84172 to 0.84175, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "143/250 [================>.............] - ETA: 1:42 - loss: 0.4520 - accuracy: 0.8417\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "144/250 [================>.............] - ETA: 1:41 - loss: 0.4526 - accuracy: 0.8411\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "145/250 [================>.............] - ETA: 1:40 - loss: 0.4535 - accuracy: 0.8411\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "146/250 [================>.............] - ETA: 1:38 - loss: 0.4555 - accuracy: 0.8403\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "147/250 [================>.............] - ETA: 1:37 - loss: 0.4548 - accuracy: 0.8405\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "148/250 [================>.............] - ETA: 1:36 - loss: 0.4557 - accuracy: 0.8397\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "149/250 [================>.............] - ETA: 1:35 - loss: 0.4570 - accuracy: 0.8393\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "150/250 [=================>............] - ETA: 1:33 - loss: 0.4575 - accuracy: 0.8393\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "151/250 [=================>............] - ETA: 1:32 - loss: 0.4571 - accuracy: 0.8396\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "152/250 [=================>............] - ETA: 1:31 - loss: 0.4576 - accuracy: 0.8394\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "153/250 [=================>............] - ETA: 1:30 - loss: 0.4584 - accuracy: 0.8386\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "154/250 [=================>............] - ETA: 1:29 - loss: 0.4578 - accuracy: 0.8388\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "155/250 [=================>............] - ETA: 1:28 - loss: 0.4573 - accuracy: 0.8389\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "156/250 [=================>............] - ETA: 1:27 - loss: 0.4570 - accuracy: 0.8389\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "157/250 [=================>............] - ETA: 1:26 - loss: 0.4560 - accuracy: 0.8395\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "158/250 [=================>............] - ETA: 1:24 - loss: 0.4544 - accuracy: 0.8403\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "159/250 [==================>...........] - ETA: 1:23 - loss: 0.4533 - accuracy: 0.8410\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "160/250 [==================>...........] - ETA: 1:22 - loss: 0.4541 - accuracy: 0.8410\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "161/250 [==================>...........] - ETA: 1:21 - loss: 0.4539 - accuracy: 0.8406\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "162/250 [==================>...........] - ETA: 1:20 - loss: 0.4545 - accuracy: 0.8404\n",
            "Epoch 2: accuracy did not improve from 0.84175\n",
            "163/250 [==================>...........] - ETA: 1:19 - loss: 0.4532 - accuracy: 0.8410\n",
            "Epoch 2: accuracy improved from 0.84175 to 0.84181, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "164/250 [==================>...........] - ETA: 1:19 - loss: 0.4520 - accuracy: 0.8418\n",
            "Epoch 2: accuracy improved from 0.84181 to 0.84202, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "165/250 [==================>...........] - ETA: 1:19 - loss: 0.4524 - accuracy: 0.8420\n",
            "Epoch 2: accuracy did not improve from 0.84202\n",
            "166/250 [==================>...........] - ETA: 1:18 - loss: 0.4554 - accuracy: 0.8411\n",
            "Epoch 2: accuracy did not improve from 0.84202\n",
            "167/250 [===================>..........] - ETA: 1:17 - loss: 0.4551 - accuracy: 0.8409\n",
            "Epoch 2: accuracy did not improve from 0.84202\n",
            "168/250 [===================>..........] - ETA: 1:16 - loss: 0.4555 - accuracy: 0.8409\n",
            "Epoch 2: accuracy did not improve from 0.84202\n",
            "169/250 [===================>..........] - ETA: 1:15 - loss: 0.4573 - accuracy: 0.8402\n",
            "Epoch 2: accuracy did not improve from 0.84202\n",
            "170/250 [===================>..........] - ETA: 1:14 - loss: 0.4568 - accuracy: 0.8404\n",
            "Epoch 2: accuracy did not improve from 0.84202\n",
            "171/250 [===================>..........] - ETA: 1:13 - loss: 0.4558 - accuracy: 0.8408\n",
            "Epoch 2: accuracy did not improve from 0.84202\n",
            "172/250 [===================>..........] - ETA: 1:11 - loss: 0.4546 - accuracy: 0.8414\n",
            "Epoch 2: accuracy did not improve from 0.84202\n",
            "173/250 [===================>..........] - ETA: 1:10 - loss: 0.4539 - accuracy: 0.8417\n",
            "Epoch 2: accuracy did not improve from 0.84202\n",
            "174/250 [===================>..........] - ETA: 1:09 - loss: 0.4537 - accuracy: 0.8416\n",
            "Epoch 2: accuracy improved from 0.84202 to 0.84211, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "175/250 [====================>.........] - ETA: 1:09 - loss: 0.4526 - accuracy: 0.8421\n",
            "Epoch 2: accuracy improved from 0.84211 to 0.84248, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "176/250 [====================>.........] - ETA: 1:09 - loss: 0.4516 - accuracy: 0.8425\n",
            "Epoch 2: accuracy did not improve from 0.84248\n",
            "177/250 [====================>.........] - ETA: 1:08 - loss: 0.4514 - accuracy: 0.8423\n",
            "Epoch 2: accuracy did not improve from 0.84248\n",
            "178/250 [====================>.........] - ETA: 1:07 - loss: 0.4529 - accuracy: 0.8421\n",
            "Epoch 2: accuracy improved from 0.84248 to 0.84268, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "179/250 [====================>.........] - ETA: 1:06 - loss: 0.4522 - accuracy: 0.8427\n",
            "Epoch 2: accuracy improved from 0.84268 to 0.84268, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "180/250 [====================>.........] - ETA: 1:06 - loss: 0.4516 - accuracy: 0.8427\n",
            "Epoch 2: accuracy improved from 0.84268 to 0.84286, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "181/250 [====================>.........] - ETA: 1:06 - loss: 0.4506 - accuracy: 0.8429\n",
            "Epoch 2: accuracy improved from 0.84286 to 0.84304, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "182/250 [====================>.........] - ETA: 1:06 - loss: 0.4496 - accuracy: 0.8430\n",
            "Epoch 2: accuracy improved from 0.84304 to 0.84321, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "183/250 [====================>.........] - ETA: 1:06 - loss: 0.4491 - accuracy: 0.8432\n",
            "Epoch 2: accuracy improved from 0.84321 to 0.84339, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "184/250 [=====================>........] - ETA: 1:05 - loss: 0.4485 - accuracy: 0.8434\n",
            "Epoch 2: accuracy did not improve from 0.84339\n",
            "185/250 [=====================>........] - ETA: 1:04 - loss: 0.4492 - accuracy: 0.8427\n",
            "Epoch 2: accuracy did not improve from 0.84339\n",
            "186/250 [=====================>........] - ETA: 1:03 - loss: 0.4484 - accuracy: 0.8431\n",
            "Epoch 2: accuracy improved from 0.84339 to 0.84356, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "187/250 [=====================>........] - ETA: 1:02 - loss: 0.4475 - accuracy: 0.8436\n",
            "Epoch 2: accuracy improved from 0.84356 to 0.84406, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "188/250 [=====================>........] - ETA: 1:02 - loss: 0.4464 - accuracy: 0.8441\n",
            "Epoch 2: accuracy improved from 0.84406 to 0.84439, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "189/250 [=====================>........] - ETA: 1:02 - loss: 0.4458 - accuracy: 0.8444\n",
            "Epoch 2: accuracy improved from 0.84439 to 0.84472, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "190/250 [=====================>........] - ETA: 1:01 - loss: 0.4455 - accuracy: 0.8447\n",
            "Epoch 2: accuracy did not improve from 0.84472\n",
            "191/250 [=====================>........] - ETA: 1:00 - loss: 0.4453 - accuracy: 0.8446\n",
            "Epoch 2: accuracy did not improve from 0.84472\n",
            "192/250 [======================>.......] - ETA: 59s - loss: 0.4455 - accuracy: 0.8444 \n",
            "Epoch 2: accuracy did not improve from 0.84472\n",
            "193/250 [======================>.......] - ETA: 58s - loss: 0.4458 - accuracy: 0.8441\n",
            "Epoch 2: accuracy did not improve from 0.84472\n",
            "194/250 [======================>.......] - ETA: 56s - loss: 0.4455 - accuracy: 0.8442\n",
            "Epoch 2: accuracy did not improve from 0.84472\n",
            "195/250 [======================>.......] - ETA: 55s - loss: 0.4455 - accuracy: 0.8442\n",
            "Epoch 2: accuracy did not improve from 0.84472\n",
            "196/250 [======================>.......] - ETA: 54s - loss: 0.4447 - accuracy: 0.8445\n",
            "Epoch 2: accuracy improved from 0.84472 to 0.84484, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "197/250 [======================>.......] - ETA: 53s - loss: 0.4440 - accuracy: 0.8448\n",
            "Epoch 2: accuracy did not improve from 0.84484\n",
            "198/250 [======================>.......] - ETA: 52s - loss: 0.4445 - accuracy: 0.8442\n",
            "Epoch 2: accuracy did not improve from 0.84484\n",
            "199/250 [======================>.......] - ETA: 51s - loss: 0.4440 - accuracy: 0.8444\n",
            "Epoch 2: accuracy did not improve from 0.84484\n",
            "200/250 [=======================>......] - ETA: 50s - loss: 0.4436 - accuracy: 0.8445\n",
            "Epoch 2: accuracy improved from 0.84484 to 0.84498, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "201/250 [=======================>......] - ETA: 49s - loss: 0.4425 - accuracy: 0.8450\n",
            "Epoch 2: accuracy improved from 0.84498 to 0.84528, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "202/250 [=======================>......] - ETA: 48s - loss: 0.4413 - accuracy: 0.8453\n",
            "Epoch 2: accuracy did not improve from 0.84528\n",
            "203/250 [=======================>......] - ETA: 47s - loss: 0.4417 - accuracy: 0.8451\n",
            "Epoch 2: accuracy improved from 0.84528 to 0.84542, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "204/250 [=======================>......] - ETA: 47s - loss: 0.4415 - accuracy: 0.8454\n",
            "Epoch 2: accuracy improved from 0.84542 to 0.84572, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "205/250 [=======================>......] - ETA: 46s - loss: 0.4410 - accuracy: 0.8457\n",
            "Epoch 2: accuracy improved from 0.84572 to 0.84647, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "206/250 [=======================>......] - ETA: 45s - loss: 0.4396 - accuracy: 0.8465\n",
            "Epoch 2: accuracy improved from 0.84647 to 0.84661, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "207/250 [=======================>......] - ETA: 45s - loss: 0.4391 - accuracy: 0.8466\n",
            "Epoch 2: accuracy did not improve from 0.84661\n",
            "208/250 [=======================>......] - ETA: 43s - loss: 0.4396 - accuracy: 0.8463\n",
            "Epoch 2: accuracy improved from 0.84661 to 0.84703, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "209/250 [========================>.....] - ETA: 43s - loss: 0.4381 - accuracy: 0.8470\n",
            "Epoch 2: accuracy improved from 0.84703 to 0.84761, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "210/250 [========================>.....] - ETA: 42s - loss: 0.4367 - accuracy: 0.8476\n",
            "Epoch 2: accuracy did not improve from 0.84761\n",
            "211/250 [========================>.....] - ETA: 41s - loss: 0.4374 - accuracy: 0.8473\n",
            "Epoch 2: accuracy did not improve from 0.84761\n",
            "212/250 [========================>.....] - ETA: 40s - loss: 0.4368 - accuracy: 0.8473\n",
            "Epoch 2: accuracy did not improve from 0.84761\n",
            "213/250 [========================>.....] - ETA: 38s - loss: 0.4361 - accuracy: 0.8476\n",
            "Epoch 2: accuracy improved from 0.84761 to 0.84783, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "214/250 [========================>.....] - ETA: 37s - loss: 0.4352 - accuracy: 0.8478\n",
            "Epoch 2: accuracy improved from 0.84783 to 0.84810, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "215/250 [========================>.....] - ETA: 36s - loss: 0.4346 - accuracy: 0.8481\n",
            "Epoch 2: accuracy improved from 0.84810 to 0.84823, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "216/250 [========================>.....] - ETA: 36s - loss: 0.4346 - accuracy: 0.8482\n",
            "Epoch 2: accuracy improved from 0.84823 to 0.84864, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "217/250 [=========================>....] - ETA: 35s - loss: 0.4336 - accuracy: 0.8486\n",
            "Epoch 2: accuracy did not improve from 0.84864\n",
            "218/250 [=========================>....] - ETA: 34s - loss: 0.4332 - accuracy: 0.8486\n",
            "Epoch 2: accuracy did not improve from 0.84864\n",
            "219/250 [=========================>....] - ETA: 33s - loss: 0.4336 - accuracy: 0.8486\n",
            "Epoch 2: accuracy did not improve from 0.84864\n",
            "220/250 [=========================>....] - ETA: 31s - loss: 0.4336 - accuracy: 0.8486\n",
            "Epoch 2: accuracy improved from 0.84864 to 0.84870, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "221/250 [=========================>....] - ETA: 31s - loss: 0.4332 - accuracy: 0.8487\n",
            "Epoch 2: accuracy improved from 0.84870 to 0.84910, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "222/250 [=========================>....] - ETA: 30s - loss: 0.4325 - accuracy: 0.8491\n",
            "Epoch 2: accuracy improved from 0.84910 to 0.84921, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "223/250 [=========================>....] - ETA: 29s - loss: 0.4326 - accuracy: 0.8492\n",
            "Epoch 2: accuracy improved from 0.84921 to 0.84947, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "224/250 [=========================>....] - ETA: 28s - loss: 0.4321 - accuracy: 0.8495\n",
            "Epoch 2: accuracy improved from 0.84947 to 0.84958, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "225/250 [==========================>...] - ETA: 27s - loss: 0.4318 - accuracy: 0.8496\n",
            "Epoch 2: accuracy improved from 0.84958 to 0.85025, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "226/250 [==========================>...] - ETA: 26s - loss: 0.4304 - accuracy: 0.8502\n",
            "Epoch 2: accuracy improved from 0.85025 to 0.85050, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "227/250 [==========================>...] - ETA: 25s - loss: 0.4298 - accuracy: 0.8505\n",
            "Epoch 2: accuracy improved from 0.85050 to 0.85088, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "228/250 [==========================>...] - ETA: 24s - loss: 0.4292 - accuracy: 0.8509\n",
            "Epoch 2: accuracy did not improve from 0.85088\n",
            "229/250 [==========================>...] - ETA: 23s - loss: 0.4288 - accuracy: 0.8508\n",
            "Epoch 2: accuracy improved from 0.85088 to 0.85095, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "230/250 [==========================>...] - ETA: 22s - loss: 0.4284 - accuracy: 0.8510\n",
            "Epoch 2: accuracy improved from 0.85095 to 0.85133, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "231/250 [==========================>...] - ETA: 21s - loss: 0.4275 - accuracy: 0.8513\n",
            "Epoch 2: accuracy did not improve from 0.85133\n",
            "232/250 [==========================>...] - ETA: 20s - loss: 0.4280 - accuracy: 0.8513\n",
            "Epoch 2: accuracy did not improve from 0.85133\n",
            "233/250 [==========================>...] - ETA: 19s - loss: 0.4278 - accuracy: 0.8510\n",
            "Epoch 2: accuracy did not improve from 0.85133\n",
            "234/250 [===========================>..] - ETA: 17s - loss: 0.4278 - accuracy: 0.8507\n",
            "Epoch 2: accuracy did not improve from 0.85133\n",
            "235/250 [===========================>..] - ETA: 16s - loss: 0.4288 - accuracy: 0.8505\n",
            "Epoch 2: accuracy did not improve from 0.85133\n",
            "236/250 [===========================>..] - ETA: 15s - loss: 0.4288 - accuracy: 0.8506\n",
            "Epoch 2: accuracy did not improve from 0.85133\n",
            "237/250 [===========================>..] - ETA: 14s - loss: 0.4278 - accuracy: 0.8510\n",
            "Epoch 2: accuracy did not improve from 0.85133\n",
            "238/250 [===========================>..] - ETA: 13s - loss: 0.4274 - accuracy: 0.8512\n",
            "Epoch 2: accuracy did not improve from 0.85133\n",
            "239/250 [===========================>..] - ETA: 12s - loss: 0.4273 - accuracy: 0.8509\n",
            "Epoch 2: accuracy did not improve from 0.85133\n",
            "240/250 [===========================>..] - ETA: 11s - loss: 0.4268 - accuracy: 0.8513\n",
            "Epoch 2: accuracy improved from 0.85133 to 0.85166, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "241/250 [===========================>..] - ETA: 9s - loss: 0.4259 - accuracy: 0.8517 \n",
            "Epoch 2: accuracy did not improve from 0.85166\n",
            "242/250 [============================>.] - ETA: 8s - loss: 0.4261 - accuracy: 0.8512\n",
            "Epoch 2: accuracy did not improve from 0.85166\n",
            "243/250 [============================>.] - ETA: 7s - loss: 0.4254 - accuracy: 0.8516\n",
            "Epoch 2: accuracy improved from 0.85166 to 0.85182, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "244/250 [============================>.] - ETA: 6s - loss: 0.4246 - accuracy: 0.8518\n",
            "Epoch 2: accuracy did not improve from 0.85182\n",
            "245/250 [============================>.] - ETA: 5s - loss: 0.4250 - accuracy: 0.8513\n",
            "Epoch 2: accuracy did not improve from 0.85182\n",
            "246/250 [============================>.] - ETA: 4s - loss: 0.4245 - accuracy: 0.8515\n",
            "Epoch 2: accuracy did not improve from 0.85182\n",
            "247/250 [============================>.] - ETA: 3s - loss: 0.4243 - accuracy: 0.8513\n",
            "Epoch 2: accuracy did not improve from 0.85182\n",
            "248/250 [============================>.] - ETA: 2s - loss: 0.4252 - accuracy: 0.8507\n",
            "Epoch 2: accuracy did not improve from 0.85182\n",
            "249/250 [============================>.] - ETA: 1s - loss: 0.4250 - accuracy: 0.8508\n",
            "Epoch 2: accuracy did not improve from 0.85182\n",
            "250/250 [==============================] - 317s 1s/step - loss: 0.4245 - accuracy: 0.8508 - val_loss: 0.2138 - val_accuracy: 0.9405\n",
            "Epoch 3/5\n",
            "\n",
            "Epoch 3: accuracy improved from 0.85182 to 0.90625, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  1/250 [..............................] - ETA: 15:39 - loss: 0.2429 - accuracy: 0.9062\n",
            "Epoch 3: accuracy improved from 0.90625 to 0.95312, saving model to /content/drive/MyDrive/Train_PCB_faults/output/resnet101.h5\n",
            "  2/250 [..............................] - ETA: 15:37 - loss: 0.1955 - accuracy: 0.9531\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "  3/250 [..............................] - ETA: 8:16 - loss: 0.2557 - accuracy: 0.9271 \n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "  4/250 [..............................] - ETA: 6:17 - loss: 0.2466 - accuracy: 0.9297\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "  5/250 [..............................] - ETA: 5:10 - loss: 0.2521 - accuracy: 0.9250\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "  6/250 [..............................] - ETA: 4:30 - loss: 0.3176 - accuracy: 0.8958\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "  7/250 [..............................] - ETA: 4:03 - loss: 0.3148 - accuracy: 0.8929\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "  8/250 [..............................] - ETA: 3:44 - loss: 0.3360 - accuracy: 0.8828\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "  9/250 [>.............................] - ETA: 3:29 - loss: 0.3366 - accuracy: 0.8750\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 10/250 [>.............................] - ETA: 3:18 - loss: 0.3292 - accuracy: 0.8781\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 11/250 [>.............................] - ETA: 3:08 - loss: 0.3204 - accuracy: 0.8778\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 12/250 [>.............................] - ETA: 3:01 - loss: 0.3131 - accuracy: 0.8828\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 13/250 [>.............................] - ETA: 2:55 - loss: 0.3098 - accuracy: 0.8846\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 14/250 [>.............................] - ETA: 2:49 - loss: 0.3055 - accuracy: 0.8862\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 15/250 [>.............................] - ETA: 2:44 - loss: 0.3037 - accuracy: 0.8875\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 16/250 [>.............................] - ETA: 2:39 - loss: 0.3108 - accuracy: 0.8906\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 17/250 [=>............................] - ETA: 2:36 - loss: 0.3118 - accuracy: 0.8934\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 18/250 [=>............................] - ETA: 2:33 - loss: 0.3128 - accuracy: 0.8941\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 19/250 [=>............................] - ETA: 2:30 - loss: 0.3096 - accuracy: 0.8931\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 20/250 [=>............................] - ETA: 2:27 - loss: 0.3048 - accuracy: 0.8938\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 21/250 [=>............................] - ETA: 2:24 - loss: 0.3064 - accuracy: 0.8914\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 22/250 [=>............................] - ETA: 2:22 - loss: 0.3083 - accuracy: 0.8920\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 23/250 [=>............................] - ETA: 2:22 - loss: 0.3068 - accuracy: 0.8940\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 24/250 [=>............................] - ETA: 2:23 - loss: 0.3085 - accuracy: 0.8932\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 25/250 [==>...........................] - ETA: 2:25 - loss: 0.3036 - accuracy: 0.8950\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 26/250 [==>...........................] - ETA: 2:27 - loss: 0.3017 - accuracy: 0.8954\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 27/250 [==>...........................] - ETA: 2:25 - loss: 0.3015 - accuracy: 0.8958\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 28/250 [==>...........................] - ETA: 2:24 - loss: 0.3081 - accuracy: 0.8929\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 29/250 [==>...........................] - ETA: 2:22 - loss: 0.3101 - accuracy: 0.8944\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 30/250 [==>...........................] - ETA: 2:20 - loss: 0.3053 - accuracy: 0.8969\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 31/250 [==>...........................] - ETA: 2:19 - loss: 0.3056 - accuracy: 0.8972\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 32/250 [==>...........................] - ETA: 2:17 - loss: 0.3077 - accuracy: 0.8965\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 33/250 [==>...........................] - ETA: 2:15 - loss: 0.3095 - accuracy: 0.8958\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 34/250 [===>..........................] - ETA: 2:14 - loss: 0.3120 - accuracy: 0.8961\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 35/250 [===>..........................] - ETA: 2:12 - loss: 0.3195 - accuracy: 0.8929\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 36/250 [===>..........................] - ETA: 2:11 - loss: 0.3144 - accuracy: 0.8958\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 37/250 [===>..........................] - ETA: 2:10 - loss: 0.3164 - accuracy: 0.8936\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 38/250 [===>..........................] - ETA: 2:09 - loss: 0.3206 - accuracy: 0.8914\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 39/250 [===>..........................] - ETA: 2:07 - loss: 0.3201 - accuracy: 0.8902\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 40/250 [===>..........................] - ETA: 2:06 - loss: 0.3233 - accuracy: 0.8891\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 41/250 [===>..........................] - ETA: 2:05 - loss: 0.3257 - accuracy: 0.8902\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 42/250 [====>.........................] - ETA: 2:04 - loss: 0.3280 - accuracy: 0.8884\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 43/250 [====>.........................] - ETA: 2:03 - loss: 0.3313 - accuracy: 0.8859\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 44/250 [====>.........................] - ETA: 2:02 - loss: 0.3331 - accuracy: 0.8842\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 45/250 [====>.........................] - ETA: 2:01 - loss: 0.3405 - accuracy: 0.8826\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 46/250 [====>.........................] - ETA: 2:00 - loss: 0.3413 - accuracy: 0.8832\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 47/250 [====>.........................] - ETA: 2:00 - loss: 0.3391 - accuracy: 0.8830\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 48/250 [====>.........................] - ETA: 2:00 - loss: 0.3424 - accuracy: 0.8822\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 49/250 [====>.........................] - ETA: 2:00 - loss: 0.3401 - accuracy: 0.8827\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 50/250 [=====>........................] - ETA: 2:00 - loss: 0.3417 - accuracy: 0.8813\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 51/250 [=====>........................] - ETA: 1:59 - loss: 0.3438 - accuracy: 0.8817\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 52/250 [=====>........................] - ETA: 1:59 - loss: 0.3416 - accuracy: 0.8828\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 53/250 [=====>........................] - ETA: 1:58 - loss: 0.3437 - accuracy: 0.8815\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 54/250 [=====>........................] - ETA: 1:58 - loss: 0.3419 - accuracy: 0.8819\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 55/250 [=====>........................] - ETA: 1:58 - loss: 0.3400 - accuracy: 0.8824\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 56/250 [=====>........................] - ETA: 1:58 - loss: 0.3397 - accuracy: 0.8823\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 57/250 [=====>........................] - ETA: 1:57 - loss: 0.3402 - accuracy: 0.8821\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 58/250 [=====>........................] - ETA: 1:56 - loss: 0.3385 - accuracy: 0.8836\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 59/250 [======>.......................] - ETA: 1:55 - loss: 0.3409 - accuracy: 0.8829\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 60/250 [======>.......................] - ETA: 1:54 - loss: 0.3408 - accuracy: 0.8833\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 61/250 [======>.......................] - ETA: 1:53 - loss: 0.3383 - accuracy: 0.8847\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 62/250 [======>.......................] - ETA: 1:52 - loss: 0.3394 - accuracy: 0.8856\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 63/250 [======>.......................] - ETA: 1:51 - loss: 0.3400 - accuracy: 0.8859\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 64/250 [======>.......................] - ETA: 1:50 - loss: 0.3412 - accuracy: 0.8853\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 65/250 [======>.......................] - ETA: 1:49 - loss: 0.3416 - accuracy: 0.8851\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 66/250 [======>.......................] - ETA: 1:48 - loss: 0.3411 - accuracy: 0.8859\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 67/250 [=======>......................] - ETA: 1:47 - loss: 0.3394 - accuracy: 0.8862\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 68/250 [=======>......................] - ETA: 1:46 - loss: 0.3365 - accuracy: 0.8879\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 69/250 [=======>......................] - ETA: 1:45 - loss: 0.3372 - accuracy: 0.8868\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 70/250 [=======>......................] - ETA: 1:45 - loss: 0.3368 - accuracy: 0.8862\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 71/250 [=======>......................] - ETA: 1:44 - loss: 0.3375 - accuracy: 0.8860\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 72/250 [=======>......................] - ETA: 1:44 - loss: 0.3396 - accuracy: 0.8854\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 73/250 [=======>......................] - ETA: 1:43 - loss: 0.3408 - accuracy: 0.8857\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 74/250 [=======>......................] - ETA: 1:43 - loss: 0.3421 - accuracy: 0.8856\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 75/250 [========>.....................] - ETA: 1:43 - loss: 0.3414 - accuracy: 0.8858\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 76/250 [========>.....................] - ETA: 1:42 - loss: 0.3462 - accuracy: 0.8840\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 77/250 [========>.....................] - ETA: 1:41 - loss: 0.3449 - accuracy: 0.8843\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 78/250 [========>.....................] - ETA: 1:40 - loss: 0.3443 - accuracy: 0.8838\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 79/250 [========>.....................] - ETA: 1:40 - loss: 0.3442 - accuracy: 0.8833\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 80/250 [========>.....................] - ETA: 1:39 - loss: 0.3417 - accuracy: 0.8844\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 81/250 [========>.....................] - ETA: 1:38 - loss: 0.3399 - accuracy: 0.8854\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 82/250 [========>.....................] - ETA: 1:37 - loss: 0.3405 - accuracy: 0.8853\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 83/250 [========>.....................] - ETA: 1:36 - loss: 0.3399 - accuracy: 0.8855\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 84/250 [=========>....................] - ETA: 1:36 - loss: 0.3399 - accuracy: 0.8854\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 85/250 [=========>....................] - ETA: 1:35 - loss: 0.3379 - accuracy: 0.8864\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 86/250 [=========>....................] - ETA: 1:34 - loss: 0.3376 - accuracy: 0.8859\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 87/250 [=========>....................] - ETA: 1:33 - loss: 0.3391 - accuracy: 0.8851\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 88/250 [=========>....................] - ETA: 1:33 - loss: 0.3415 - accuracy: 0.8842\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 89/250 [=========>....................] - ETA: 1:32 - loss: 0.3416 - accuracy: 0.8848\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 90/250 [=========>....................] - ETA: 1:31 - loss: 0.3443 - accuracy: 0.8840\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 91/250 [=========>....................] - ETA: 1:30 - loss: 0.3445 - accuracy: 0.8836\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 92/250 [==========>...................] - ETA: 1:30 - loss: 0.3448 - accuracy: 0.8835\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 93/250 [==========>...................] - ETA: 1:29 - loss: 0.3481 - accuracy: 0.8827\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 94/250 [==========>...................] - ETA: 1:28 - loss: 0.3470 - accuracy: 0.8833\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 95/250 [==========>...................] - ETA: 1:27 - loss: 0.3465 - accuracy: 0.8836\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 96/250 [==========>...................] - ETA: 1:27 - loss: 0.3447 - accuracy: 0.8841\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 97/250 [==========>...................] - ETA: 1:26 - loss: 0.3432 - accuracy: 0.8847\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 98/250 [==========>...................] - ETA: 1:26 - loss: 0.3427 - accuracy: 0.8842\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            " 99/250 [==========>...................] - ETA: 1:25 - loss: 0.3422 - accuracy: 0.8844\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "100/250 [===========>..................] - ETA: 1:24 - loss: 0.3439 - accuracy: 0.8830\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "101/250 [===========>..................] - ETA: 1:24 - loss: 0.3468 - accuracy: 0.8823\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "102/250 [===========>..................] - ETA: 1:24 - loss: 0.3470 - accuracy: 0.8819\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "103/250 [===========>..................] - ETA: 1:23 - loss: 0.3473 - accuracy: 0.8813\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "104/250 [===========>..................] - ETA: 1:22 - loss: 0.3489 - accuracy: 0.8809\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "105/250 [===========>..................] - ETA: 1:22 - loss: 0.3484 - accuracy: 0.8805\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "106/250 [===========>..................] - ETA: 1:21 - loss: 0.3483 - accuracy: 0.8805\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "107/250 [===========>..................] - ETA: 1:20 - loss: 0.3482 - accuracy: 0.8807\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "108/250 [===========>..................] - ETA: 1:20 - loss: 0.3492 - accuracy: 0.8804\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "109/250 [============>.................] - ETA: 1:19 - loss: 0.3481 - accuracy: 0.8809\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "110/250 [============>.................] - ETA: 1:18 - loss: 0.3489 - accuracy: 0.8809\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "111/250 [============>.................] - ETA: 1:18 - loss: 0.3492 - accuracy: 0.8808\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "112/250 [============>.................] - ETA: 1:17 - loss: 0.3491 - accuracy: 0.8810\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "113/250 [============>.................] - ETA: 1:16 - loss: 0.3490 - accuracy: 0.8815\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "114/250 [============>.................] - ETA: 1:16 - loss: 0.3501 - accuracy: 0.8807\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "115/250 [============>.................] - ETA: 1:15 - loss: 0.3488 - accuracy: 0.8809\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "116/250 [============>.................] - ETA: 1:14 - loss: 0.3504 - accuracy: 0.8806\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "117/250 [=============>................] - ETA: 1:14 - loss: 0.3514 - accuracy: 0.8805\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "118/250 [=============>................] - ETA: 1:13 - loss: 0.3514 - accuracy: 0.8807\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "119/250 [=============>................] - ETA: 1:12 - loss: 0.3522 - accuracy: 0.8809\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "120/250 [=============>................] - ETA: 1:12 - loss: 0.3533 - accuracy: 0.8801\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "121/250 [=============>................] - ETA: 1:11 - loss: 0.3520 - accuracy: 0.8806\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "122/250 [=============>................] - ETA: 1:10 - loss: 0.3507 - accuracy: 0.8811\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "123/250 [=============>................] - ETA: 1:10 - loss: 0.3508 - accuracy: 0.8810\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "124/250 [=============>................] - ETA: 1:09 - loss: 0.3502 - accuracy: 0.8810\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "125/250 [==============>...............] - ETA: 1:09 - loss: 0.3501 - accuracy: 0.8809\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "126/250 [==============>...............] - ETA: 1:08 - loss: 0.3496 - accuracy: 0.8814\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "127/250 [==============>...............] - ETA: 1:08 - loss: 0.3486 - accuracy: 0.8818\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "128/250 [==============>...............] - ETA: 1:08 - loss: 0.3490 - accuracy: 0.8813\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "129/250 [==============>...............] - ETA: 1:07 - loss: 0.3480 - accuracy: 0.8819\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "130/250 [==============>...............] - ETA: 1:06 - loss: 0.3492 - accuracy: 0.8816\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "131/250 [==============>...............] - ETA: 1:06 - loss: 0.3482 - accuracy: 0.8821\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "132/250 [==============>...............] - ETA: 1:05 - loss: 0.3488 - accuracy: 0.8823\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "133/250 [==============>...............] - ETA: 1:05 - loss: 0.3488 - accuracy: 0.8824\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "134/250 [===============>..............] - ETA: 1:04 - loss: 0.3488 - accuracy: 0.8824\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "135/250 [===============>..............] - ETA: 1:03 - loss: 0.3495 - accuracy: 0.8821\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "136/250 [===============>..............] - ETA: 1:03 - loss: 0.3491 - accuracy: 0.8818\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "137/250 [===============>..............] - ETA: 1:02 - loss: 0.3483 - accuracy: 0.8818\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "138/250 [===============>..............] - ETA: 1:02 - loss: 0.3482 - accuracy: 0.8815\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "139/250 [===============>..............] - ETA: 1:01 - loss: 0.3503 - accuracy: 0.8812\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "140/250 [===============>..............] - ETA: 1:00 - loss: 0.3498 - accuracy: 0.8814\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "141/250 [===============>..............] - ETA: 1:00 - loss: 0.3510 - accuracy: 0.8809\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "142/250 [================>.............] - ETA: 59s - loss: 0.3515 - accuracy: 0.8806 \n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "143/250 [================>.............] - ETA: 59s - loss: 0.3525 - accuracy: 0.8802\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "144/250 [================>.............] - ETA: 58s - loss: 0.3519 - accuracy: 0.8801\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "145/250 [================>.............] - ETA: 57s - loss: 0.3530 - accuracy: 0.8799\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "146/250 [================>.............] - ETA: 57s - loss: 0.3536 - accuracy: 0.8794\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "147/250 [================>.............] - ETA: 56s - loss: 0.3544 - accuracy: 0.8792\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "148/250 [================>.............] - ETA: 56s - loss: 0.3538 - accuracy: 0.8796\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "149/250 [================>.............] - ETA: 55s - loss: 0.3538 - accuracy: 0.8795\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "150/250 [=================>............] - ETA: 55s - loss: 0.3537 - accuracy: 0.8795\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "151/250 [=================>............] - ETA: 54s - loss: 0.3531 - accuracy: 0.8799\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "152/250 [=================>............] - ETA: 54s - loss: 0.3531 - accuracy: 0.8799\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "153/250 [=================>............] - ETA: 53s - loss: 0.3528 - accuracy: 0.8798\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "154/250 [=================>............] - ETA: 53s - loss: 0.3532 - accuracy: 0.8800\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "155/250 [=================>............] - ETA: 52s - loss: 0.3544 - accuracy: 0.8796\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "156/250 [=================>............] - ETA: 52s - loss: 0.3550 - accuracy: 0.8795\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "157/250 [=================>............] - ETA: 51s - loss: 0.3550 - accuracy: 0.8793\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "158/250 [=================>............] - ETA: 50s - loss: 0.3551 - accuracy: 0.8797\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "159/250 [==================>...........] - ETA: 50s - loss: 0.3548 - accuracy: 0.8800\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "160/250 [==================>...........] - ETA: 49s - loss: 0.3548 - accuracy: 0.8798\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "161/250 [==================>...........] - ETA: 49s - loss: 0.3553 - accuracy: 0.8798\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "162/250 [==================>...........] - ETA: 48s - loss: 0.3542 - accuracy: 0.8801\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "163/250 [==================>...........] - ETA: 48s - loss: 0.3542 - accuracy: 0.8801\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "164/250 [==================>...........] - ETA: 47s - loss: 0.3540 - accuracy: 0.8805\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "165/250 [==================>...........] - ETA: 46s - loss: 0.3535 - accuracy: 0.8804\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "166/250 [==================>...........] - ETA: 46s - loss: 0.3546 - accuracy: 0.8804\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "167/250 [===================>..........] - ETA: 45s - loss: 0.3547 - accuracy: 0.8802\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "168/250 [===================>..........] - ETA: 45s - loss: 0.3543 - accuracy: 0.8805\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "169/250 [===================>..........] - ETA: 44s - loss: 0.3535 - accuracy: 0.8807\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "170/250 [===================>..........] - ETA: 43s - loss: 0.3535 - accuracy: 0.8804\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "171/250 [===================>..........] - ETA: 43s - loss: 0.3523 - accuracy: 0.8811\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "172/250 [===================>..........] - ETA: 42s - loss: 0.3511 - accuracy: 0.8817\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "173/250 [===================>..........] - ETA: 42s - loss: 0.3504 - accuracy: 0.8818\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "174/250 [===================>..........] - ETA: 41s - loss: 0.3495 - accuracy: 0.8821\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "175/250 [====================>.........] - ETA: 41s - loss: 0.3492 - accuracy: 0.8824\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "176/250 [====================>.........] - ETA: 40s - loss: 0.3490 - accuracy: 0.8824\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "177/250 [====================>.........] - ETA: 40s - loss: 0.3483 - accuracy: 0.8827\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "178/250 [====================>.........] - ETA: 39s - loss: 0.3497 - accuracy: 0.8825\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "179/250 [====================>.........] - ETA: 39s - loss: 0.3493 - accuracy: 0.8826\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "180/250 [====================>.........] - ETA: 38s - loss: 0.3488 - accuracy: 0.8829\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "181/250 [====================>.........] - ETA: 38s - loss: 0.3493 - accuracy: 0.8825\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "182/250 [====================>.........] - ETA: 37s - loss: 0.3487 - accuracy: 0.8828\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "183/250 [====================>.........] - ETA: 36s - loss: 0.3490 - accuracy: 0.8828\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "184/250 [=====================>........] - ETA: 36s - loss: 0.3488 - accuracy: 0.8831\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "185/250 [=====================>........] - ETA: 35s - loss: 0.3489 - accuracy: 0.8832\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "186/250 [=====================>........] - ETA: 35s - loss: 0.3489 - accuracy: 0.8835\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "187/250 [=====================>........] - ETA: 34s - loss: 0.3479 - accuracy: 0.8838\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "188/250 [=====================>........] - ETA: 34s - loss: 0.3472 - accuracy: 0.8841\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "189/250 [=====================>........] - ETA: 33s - loss: 0.3468 - accuracy: 0.8842\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "190/250 [=====================>........] - ETA: 32s - loss: 0.3470 - accuracy: 0.8843\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "191/250 [=====================>........] - ETA: 32s - loss: 0.3465 - accuracy: 0.8846\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "192/250 [======================>.......] - ETA: 31s - loss: 0.3458 - accuracy: 0.8849\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "193/250 [======================>.......] - ETA: 31s - loss: 0.3450 - accuracy: 0.8853\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "194/250 [======================>.......] - ETA: 30s - loss: 0.3447 - accuracy: 0.8854\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "195/250 [======================>.......] - ETA: 30s - loss: 0.3439 - accuracy: 0.8855\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "196/250 [======================>.......] - ETA: 29s - loss: 0.3452 - accuracy: 0.8852\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "197/250 [======================>.......] - ETA: 28s - loss: 0.3444 - accuracy: 0.8854\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "198/250 [======================>.......] - ETA: 28s - loss: 0.3454 - accuracy: 0.8847\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "199/250 [======================>.......] - ETA: 27s - loss: 0.3450 - accuracy: 0.8850\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "200/250 [=======================>......] - ETA: 27s - loss: 0.3451 - accuracy: 0.8850\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "201/250 [=======================>......] - ETA: 26s - loss: 0.3448 - accuracy: 0.8851\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "202/250 [=======================>......] - ETA: 26s - loss: 0.3444 - accuracy: 0.8852\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "203/250 [=======================>......] - ETA: 25s - loss: 0.3442 - accuracy: 0.8853\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "204/250 [=======================>......] - ETA: 25s - loss: 0.3446 - accuracy: 0.8852\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "205/250 [=======================>......] - ETA: 24s - loss: 0.3453 - accuracy: 0.8846\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "206/250 [=======================>......] - ETA: 24s - loss: 0.3445 - accuracy: 0.8850\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "207/250 [=======================>......] - ETA: 23s - loss: 0.3440 - accuracy: 0.8852\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "208/250 [=======================>......] - ETA: 23s - loss: 0.3451 - accuracy: 0.8849\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "209/250 [========================>.....] - ETA: 22s - loss: 0.3448 - accuracy: 0.8853\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "210/250 [========================>.....] - ETA: 21s - loss: 0.3439 - accuracy: 0.8855\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "211/250 [========================>.....] - ETA: 21s - loss: 0.3436 - accuracy: 0.8855\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "212/250 [========================>.....] - ETA: 20s - loss: 0.3439 - accuracy: 0.8856\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "213/250 [========================>.....] - ETA: 20s - loss: 0.3436 - accuracy: 0.8857\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "214/250 [========================>.....] - ETA: 19s - loss: 0.3432 - accuracy: 0.8855\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "215/250 [========================>.....] - ETA: 19s - loss: 0.3429 - accuracy: 0.8856\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "216/250 [========================>.....] - ETA: 18s - loss: 0.3422 - accuracy: 0.8860\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "217/250 [=========================>....] - ETA: 18s - loss: 0.3420 - accuracy: 0.8859\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "218/250 [=========================>....] - ETA: 17s - loss: 0.3417 - accuracy: 0.8857\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "219/250 [=========================>....] - ETA: 16s - loss: 0.3409 - accuracy: 0.8859\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "220/250 [=========================>....] - ETA: 16s - loss: 0.3401 - accuracy: 0.8863\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "221/250 [=========================>....] - ETA: 15s - loss: 0.3403 - accuracy: 0.8860\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "222/250 [=========================>....] - ETA: 15s - loss: 0.3392 - accuracy: 0.8865\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "223/250 [=========================>....] - ETA: 14s - loss: 0.3387 - accuracy: 0.8866\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "224/250 [=========================>....] - ETA: 14s - loss: 0.3380 - accuracy: 0.8868\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "225/250 [==========================>...] - ETA: 13s - loss: 0.3383 - accuracy: 0.8866\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "226/250 [==========================>...] - ETA: 13s - loss: 0.3383 - accuracy: 0.8866\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "227/250 [==========================>...] - ETA: 12s - loss: 0.3392 - accuracy: 0.8861\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "228/250 [==========================>...] - ETA: 12s - loss: 0.3396 - accuracy: 0.8861\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "229/250 [==========================>...] - ETA: 11s - loss: 0.3392 - accuracy: 0.8862\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "230/250 [==========================>...] - ETA: 10s - loss: 0.3393 - accuracy: 0.8861\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "231/250 [==========================>...] - ETA: 10s - loss: 0.3387 - accuracy: 0.8862\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "232/250 [==========================>...] - ETA: 9s - loss: 0.3388 - accuracy: 0.8860 \n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "233/250 [==========================>...] - ETA: 9s - loss: 0.3386 - accuracy: 0.8860\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "234/250 [===========================>..] - ETA: 8s - loss: 0.3391 - accuracy: 0.8860\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "235/250 [===========================>..] - ETA: 8s - loss: 0.3388 - accuracy: 0.8861\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "236/250 [===========================>..] - ETA: 7s - loss: 0.3385 - accuracy: 0.8862\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "237/250 [===========================>..] - ETA: 7s - loss: 0.3383 - accuracy: 0.8860\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "238/250 [===========================>..] - ETA: 6s - loss: 0.3379 - accuracy: 0.8863\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "239/250 [===========================>..] - ETA: 6s - loss: 0.3376 - accuracy: 0.8866\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "240/250 [===========================>..] - ETA: 5s - loss: 0.3377 - accuracy: 0.8864\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "241/250 [===========================>..] - ETA: 4s - loss: 0.3383 - accuracy: 0.8864\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "242/250 [============================>.] - ETA: 4s - loss: 0.3398 - accuracy: 0.8858\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "243/250 [============================>.] - ETA: 3s - loss: 0.3413 - accuracy: 0.8854\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "244/250 [============================>.] - ETA: 3s - loss: 0.3407 - accuracy: 0.8855\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "245/250 [============================>.] - ETA: 2s - loss: 0.3406 - accuracy: 0.8854\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "246/250 [============================>.] - ETA: 2s - loss: 0.3400 - accuracy: 0.8858\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "247/250 [============================>.] - ETA: 1s - loss: 0.3401 - accuracy: 0.8857\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "248/250 [============================>.] - ETA: 1s - loss: 0.3402 - accuracy: 0.8859\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8858\n",
            "Epoch 3: accuracy did not improve from 0.95312\n",
            "250/250 [==============================] - 180s 709ms/step - loss: 0.3401 - accuracy: 0.8857 - val_loss: 0.2026 - val_accuracy: 0.9425\n",
            "Epoch 4/5\n",
            "\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "  1/250 [..............................] - ETA: 3:51 - loss: 0.3161 - accuracy: 0.8750\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "  2/250 [..............................] - ETA: 2:51 - loss: 0.2311 - accuracy: 0.9219\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "  3/250 [..............................] - ETA: 2:53 - loss: 0.2392 - accuracy: 0.9167\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "  4/250 [..............................] - ETA: 2:56 - loss: 0.2938 - accuracy: 0.8984\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "  5/250 [..............................] - ETA: 2:42 - loss: 0.2867 - accuracy: 0.9125\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "  6/250 [..............................] - ETA: 2:32 - loss: 0.2645 - accuracy: 0.9167\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "  7/250 [..............................] - ETA: 2:26 - loss: 0.2945 - accuracy: 0.9107\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "  8/250 [..............................] - ETA: 2:20 - loss: 0.3179 - accuracy: 0.9023\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "  9/250 [>.............................] - ETA: 2:17 - loss: 0.3021 - accuracy: 0.9062\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 10/250 [>.............................] - ETA: 2:14 - loss: 0.2852 - accuracy: 0.9156\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 11/250 [>.............................] - ETA: 2:12 - loss: 0.2960 - accuracy: 0.9091\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 12/250 [>.............................] - ETA: 2:10 - loss: 0.3027 - accuracy: 0.9062\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 13/250 [>.............................] - ETA: 2:08 - loss: 0.2985 - accuracy: 0.9087\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 14/250 [>.............................] - ETA: 2:06 - loss: 0.2919 - accuracy: 0.9107\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 15/250 [>.............................] - ETA: 2:05 - loss: 0.3002 - accuracy: 0.9083\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 16/250 [>.............................] - ETA: 2:04 - loss: 0.2922 - accuracy: 0.9082\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 17/250 [=>............................] - ETA: 2:03 - loss: 0.2959 - accuracy: 0.9026\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 18/250 [=>............................] - ETA: 2:01 - loss: 0.3010 - accuracy: 0.9028\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 19/250 [=>............................] - ETA: 2:00 - loss: 0.3065 - accuracy: 0.8997\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 20/250 [=>............................] - ETA: 1:59 - loss: 0.3010 - accuracy: 0.9031\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 21/250 [=>............................] - ETA: 1:58 - loss: 0.3041 - accuracy: 0.9018\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 22/250 [=>............................] - ETA: 1:58 - loss: 0.3006 - accuracy: 0.9034\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 23/250 [=>............................] - ETA: 1:57 - loss: 0.3050 - accuracy: 0.9022\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 24/250 [=>............................] - ETA: 1:56 - loss: 0.3081 - accuracy: 0.9023\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 25/250 [==>...........................] - ETA: 1:55 - loss: 0.3023 - accuracy: 0.9050\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 26/250 [==>...........................] - ETA: 1:57 - loss: 0.3018 - accuracy: 0.9026\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 27/250 [==>...........................] - ETA: 1:58 - loss: 0.2987 - accuracy: 0.9039\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 28/250 [==>...........................] - ETA: 1:59 - loss: 0.2958 - accuracy: 0.9051\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 29/250 [==>...........................] - ETA: 2:00 - loss: 0.2894 - accuracy: 0.9073\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 30/250 [==>...........................] - ETA: 2:00 - loss: 0.2884 - accuracy: 0.9083\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 31/250 [==>...........................] - ETA: 2:00 - loss: 0.2882 - accuracy: 0.9083\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 32/250 [==>...........................] - ETA: 1:58 - loss: 0.2897 - accuracy: 0.9082\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 33/250 [==>...........................] - ETA: 1:57 - loss: 0.2921 - accuracy: 0.9053\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 34/250 [===>..........................] - ETA: 1:57 - loss: 0.2883 - accuracy: 0.9072\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 35/250 [===>..........................] - ETA: 1:55 - loss: 0.2931 - accuracy: 0.9036\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 36/250 [===>..........................] - ETA: 1:55 - loss: 0.2965 - accuracy: 0.9019\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 37/250 [===>..........................] - ETA: 1:54 - loss: 0.3018 - accuracy: 0.8995\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 38/250 [===>..........................] - ETA: 1:53 - loss: 0.3026 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 39/250 [===>..........................] - ETA: 1:52 - loss: 0.3005 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 40/250 [===>..........................] - ETA: 1:52 - loss: 0.2978 - accuracy: 0.9016\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 41/250 [===>..........................] - ETA: 1:51 - loss: 0.2989 - accuracy: 0.9002\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 42/250 [====>.........................] - ETA: 1:50 - loss: 0.2994 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 43/250 [====>.........................] - ETA: 1:49 - loss: 0.3028 - accuracy: 0.9004\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 44/250 [====>.........................] - ETA: 1:49 - loss: 0.3015 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 45/250 [====>.........................] - ETA: 1:48 - loss: 0.3056 - accuracy: 0.9014\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 46/250 [====>.........................] - ETA: 1:47 - loss: 0.3030 - accuracy: 0.9035\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 47/250 [====>.........................] - ETA: 1:46 - loss: 0.3057 - accuracy: 0.9016\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 48/250 [====>.........................] - ETA: 1:46 - loss: 0.3057 - accuracy: 0.9017\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 49/250 [====>.........................] - ETA: 1:45 - loss: 0.3057 - accuracy: 0.9018\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 50/250 [=====>........................] - ETA: 1:45 - loss: 0.3089 - accuracy: 0.9013\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 51/250 [=====>........................] - ETA: 1:44 - loss: 0.3080 - accuracy: 0.9001\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 52/250 [=====>........................] - ETA: 1:45 - loss: 0.3090 - accuracy: 0.9002\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 53/250 [=====>........................] - ETA: 1:45 - loss: 0.3088 - accuracy: 0.9004\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 54/250 [=====>........................] - ETA: 1:45 - loss: 0.3088 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 55/250 [=====>........................] - ETA: 1:45 - loss: 0.3069 - accuracy: 0.9011\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 56/250 [=====>........................] - ETA: 1:45 - loss: 0.3034 - accuracy: 0.9023\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 57/250 [=====>........................] - ETA: 1:44 - loss: 0.3082 - accuracy: 0.9008\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 58/250 [=====>........................] - ETA: 1:43 - loss: 0.3079 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 59/250 [======>.......................] - ETA: 1:43 - loss: 0.3074 - accuracy: 0.8988\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 60/250 [======>.......................] - ETA: 1:42 - loss: 0.3065 - accuracy: 0.8990\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 61/250 [======>.......................] - ETA: 1:41 - loss: 0.3037 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 62/250 [======>.......................] - ETA: 1:41 - loss: 0.3062 - accuracy: 0.8997\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 63/250 [======>.......................] - ETA: 1:41 - loss: 0.3033 - accuracy: 0.9008\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 64/250 [======>.......................] - ETA: 1:42 - loss: 0.3033 - accuracy: 0.9009\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 65/250 [======>.......................] - ETA: 1:42 - loss: 0.3039 - accuracy: 0.9000\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 66/250 [======>.......................] - ETA: 1:42 - loss: 0.3045 - accuracy: 0.8996\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 67/250 [=======>......................] - ETA: 1:42 - loss: 0.3054 - accuracy: 0.8993\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 68/250 [=======>......................] - ETA: 1:42 - loss: 0.3041 - accuracy: 0.8989\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 69/250 [=======>......................] - ETA: 1:42 - loss: 0.3050 - accuracy: 0.8990\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 70/250 [=======>......................] - ETA: 1:42 - loss: 0.3039 - accuracy: 0.9000\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 71/250 [=======>......................] - ETA: 1:41 - loss: 0.3030 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 72/250 [=======>......................] - ETA: 1:41 - loss: 0.3002 - accuracy: 0.9015\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 73/250 [=======>......................] - ETA: 1:41 - loss: 0.2970 - accuracy: 0.9028\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 74/250 [=======>......................] - ETA: 1:41 - loss: 0.2974 - accuracy: 0.9020\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 75/250 [========>.....................] - ETA: 1:40 - loss: 0.2998 - accuracy: 0.9013\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 76/250 [========>.....................] - ETA: 1:40 - loss: 0.3019 - accuracy: 0.9013\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 77/250 [========>.....................] - ETA: 1:39 - loss: 0.3047 - accuracy: 0.9002\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 78/250 [========>.....................] - ETA: 1:39 - loss: 0.3064 - accuracy: 0.8994\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 79/250 [========>.....................] - ETA: 1:38 - loss: 0.3062 - accuracy: 0.8991\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 80/250 [========>.....................] - ETA: 1:37 - loss: 0.3049 - accuracy: 0.8996\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 81/250 [========>.....................] - ETA: 1:37 - loss: 0.3040 - accuracy: 0.8997\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 82/250 [========>.....................] - ETA: 1:36 - loss: 0.3054 - accuracy: 0.8990\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 83/250 [========>.....................] - ETA: 1:35 - loss: 0.3042 - accuracy: 0.8998\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 84/250 [=========>....................] - ETA: 1:34 - loss: 0.3037 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 85/250 [=========>....................] - ETA: 1:34 - loss: 0.3030 - accuracy: 0.9004\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 86/250 [=========>....................] - ETA: 1:33 - loss: 0.3009 - accuracy: 0.9012\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 87/250 [=========>....................] - ETA: 1:32 - loss: 0.3002 - accuracy: 0.9012\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 88/250 [=========>....................] - ETA: 1:32 - loss: 0.3010 - accuracy: 0.9009\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 89/250 [=========>....................] - ETA: 1:31 - loss: 0.3007 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 90/250 [=========>....................] - ETA: 1:30 - loss: 0.3025 - accuracy: 0.8997\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 91/250 [=========>....................] - ETA: 1:30 - loss: 0.3026 - accuracy: 0.8990\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 92/250 [==========>...................] - ETA: 1:29 - loss: 0.3004 - accuracy: 0.8998\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 93/250 [==========>...................] - ETA: 1:28 - loss: 0.2994 - accuracy: 0.9002\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 94/250 [==========>...................] - ETA: 1:28 - loss: 0.2974 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 95/250 [==========>...................] - ETA: 1:27 - loss: 0.2966 - accuracy: 0.9007\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 96/250 [==========>...................] - ETA: 1:26 - loss: 0.2966 - accuracy: 0.8997\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 97/250 [==========>...................] - ETA: 1:26 - loss: 0.2972 - accuracy: 0.8995\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 98/250 [==========>...................] - ETA: 1:26 - loss: 0.2983 - accuracy: 0.8992\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            " 99/250 [==========>...................] - ETA: 1:25 - loss: 0.2981 - accuracy: 0.8996\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "100/250 [===========>..................] - ETA: 1:25 - loss: 0.2979 - accuracy: 0.8994\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "101/250 [===========>..................] - ETA: 1:25 - loss: 0.2985 - accuracy: 0.8991\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "102/250 [===========>..................] - ETA: 1:24 - loss: 0.2987 - accuracy: 0.8989\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "103/250 [===========>..................] - ETA: 1:23 - loss: 0.3014 - accuracy: 0.8981\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "104/250 [===========>..................] - ETA: 1:23 - loss: 0.2994 - accuracy: 0.8990\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "105/250 [===========>..................] - ETA: 1:22 - loss: 0.3002 - accuracy: 0.8988\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "106/250 [===========>..................] - ETA: 1:21 - loss: 0.3019 - accuracy: 0.8977\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "107/250 [===========>..................] - ETA: 1:20 - loss: 0.3005 - accuracy: 0.8981\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "108/250 [===========>..................] - ETA: 1:20 - loss: 0.3008 - accuracy: 0.8981\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "109/250 [============>.................] - ETA: 1:19 - loss: 0.3002 - accuracy: 0.8985\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "110/250 [============>.................] - ETA: 1:18 - loss: 0.2998 - accuracy: 0.8989\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "111/250 [============>.................] - ETA: 1:18 - loss: 0.2992 - accuracy: 0.8986\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "112/250 [============>.................] - ETA: 1:17 - loss: 0.2986 - accuracy: 0.8987\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "113/250 [============>.................] - ETA: 1:16 - loss: 0.2977 - accuracy: 0.8991\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "114/250 [============>.................] - ETA: 1:16 - loss: 0.2960 - accuracy: 0.8997\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "115/250 [============>.................] - ETA: 1:15 - loss: 0.2977 - accuracy: 0.8989\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "116/250 [============>.................] - ETA: 1:14 - loss: 0.2989 - accuracy: 0.8984\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "117/250 [=============>................] - ETA: 1:14 - loss: 0.3002 - accuracy: 0.8982\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "118/250 [=============>................] - ETA: 1:13 - loss: 0.2984 - accuracy: 0.8988\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "119/250 [=============>................] - ETA: 1:13 - loss: 0.2996 - accuracy: 0.8981\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "120/250 [=============>................] - ETA: 1:12 - loss: 0.3029 - accuracy: 0.8971\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "121/250 [=============>................] - ETA: 1:11 - loss: 0.3022 - accuracy: 0.8972\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "122/250 [=============>................] - ETA: 1:11 - loss: 0.3019 - accuracy: 0.8973\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "123/250 [=============>................] - ETA: 1:10 - loss: 0.3008 - accuracy: 0.8976\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "124/250 [=============>................] - ETA: 1:10 - loss: 0.3010 - accuracy: 0.8972\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "125/250 [==============>...............] - ETA: 1:10 - loss: 0.3003 - accuracy: 0.8975\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "126/250 [==============>...............] - ETA: 1:09 - loss: 0.3016 - accuracy: 0.8968\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "127/250 [==============>...............] - ETA: 1:09 - loss: 0.3005 - accuracy: 0.8974\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "128/250 [==============>...............] - ETA: 1:08 - loss: 0.3009 - accuracy: 0.8975\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "129/250 [==============>...............] - ETA: 1:07 - loss: 0.3028 - accuracy: 0.8968\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "130/250 [==============>...............] - ETA: 1:07 - loss: 0.3016 - accuracy: 0.8971\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "131/250 [==============>...............] - ETA: 1:06 - loss: 0.3006 - accuracy: 0.8974\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "132/250 [==============>...............] - ETA: 1:06 - loss: 0.3019 - accuracy: 0.8973\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "133/250 [==============>...............] - ETA: 1:05 - loss: 0.3022 - accuracy: 0.8969\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "134/250 [===============>..............] - ETA: 1:04 - loss: 0.3014 - accuracy: 0.8969\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "135/250 [===============>..............] - ETA: 1:04 - loss: 0.3007 - accuracy: 0.8972\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "136/250 [===============>..............] - ETA: 1:03 - loss: 0.3001 - accuracy: 0.8973\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "137/250 [===============>..............] - ETA: 1:02 - loss: 0.2992 - accuracy: 0.8974\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "138/250 [===============>..............] - ETA: 1:02 - loss: 0.2988 - accuracy: 0.8976\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "139/250 [===============>..............] - ETA: 1:01 - loss: 0.2984 - accuracy: 0.8977\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "140/250 [===============>..............] - ETA: 1:01 - loss: 0.2978 - accuracy: 0.8980\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "141/250 [===============>..............] - ETA: 1:00 - loss: 0.2967 - accuracy: 0.8987\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "142/250 [================>.............] - ETA: 59s - loss: 0.2975 - accuracy: 0.8988 \n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "143/250 [================>.............] - ETA: 59s - loss: 0.2967 - accuracy: 0.8990\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "144/250 [================>.............] - ETA: 58s - loss: 0.2972 - accuracy: 0.8989\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "145/250 [================>.............] - ETA: 58s - loss: 0.2968 - accuracy: 0.8994\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "146/250 [================>.............] - ETA: 57s - loss: 0.2970 - accuracy: 0.8994\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "147/250 [================>.............] - ETA: 56s - loss: 0.2959 - accuracy: 0.8999\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "148/250 [================>.............] - ETA: 56s - loss: 0.2955 - accuracy: 0.9001\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "149/250 [================>.............] - ETA: 55s - loss: 0.2953 - accuracy: 0.9004\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "150/250 [=================>............] - ETA: 55s - loss: 0.2956 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "151/250 [=================>............] - ETA: 55s - loss: 0.2953 - accuracy: 0.9007\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "152/250 [=================>............] - ETA: 54s - loss: 0.2949 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "153/250 [=================>............] - ETA: 54s - loss: 0.2944 - accuracy: 0.9009\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "154/250 [=================>............] - ETA: 53s - loss: 0.2942 - accuracy: 0.9012\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "155/250 [=================>............] - ETA: 52s - loss: 0.2945 - accuracy: 0.9010\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "156/250 [=================>............] - ETA: 52s - loss: 0.2939 - accuracy: 0.9012\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "157/250 [=================>............] - ETA: 51s - loss: 0.2950 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "158/250 [=================>............] - ETA: 51s - loss: 0.2945 - accuracy: 0.9009\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "159/250 [==================>...........] - ETA: 50s - loss: 0.2946 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "160/250 [==================>...........] - ETA: 49s - loss: 0.2946 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "161/250 [==================>...........] - ETA: 49s - loss: 0.2976 - accuracy: 0.8998\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "162/250 [==================>...........] - ETA: 48s - loss: 0.2964 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "163/250 [==================>...........] - ETA: 48s - loss: 0.2972 - accuracy: 0.8999\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "164/250 [==================>...........] - ETA: 47s - loss: 0.2980 - accuracy: 0.8996\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "165/250 [==================>...........] - ETA: 46s - loss: 0.2980 - accuracy: 0.8996\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "166/250 [==================>...........] - ETA: 46s - loss: 0.2966 - accuracy: 0.9002\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "167/250 [===================>..........] - ETA: 45s - loss: 0.2966 - accuracy: 0.8999\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "168/250 [===================>..........] - ETA: 45s - loss: 0.2960 - accuracy: 0.8999\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "169/250 [===================>..........] - ETA: 44s - loss: 0.2955 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "170/250 [===================>..........] - ETA: 44s - loss: 0.2956 - accuracy: 0.9002\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "171/250 [===================>..........] - ETA: 43s - loss: 0.2949 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "172/250 [===================>..........] - ETA: 42s - loss: 0.2951 - accuracy: 0.9008\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "173/250 [===================>..........] - ETA: 42s - loss: 0.2949 - accuracy: 0.9008\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "174/250 [===================>..........] - ETA: 41s - loss: 0.2940 - accuracy: 0.9012\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "175/250 [====================>.........] - ETA: 41s - loss: 0.2938 - accuracy: 0.9014\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "176/250 [====================>.........] - ETA: 40s - loss: 0.2927 - accuracy: 0.9018\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "177/250 [====================>.........] - ETA: 40s - loss: 0.2929 - accuracy: 0.9013\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "178/250 [====================>.........] - ETA: 39s - loss: 0.2930 - accuracy: 0.9013\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "179/250 [====================>.........] - ETA: 39s - loss: 0.2925 - accuracy: 0.9012\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "180/250 [====================>.........] - ETA: 38s - loss: 0.2926 - accuracy: 0.9010\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "181/250 [====================>.........] - ETA: 38s - loss: 0.2923 - accuracy: 0.9009\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "182/250 [====================>.........] - ETA: 37s - loss: 0.2927 - accuracy: 0.9009\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "183/250 [====================>.........] - ETA: 37s - loss: 0.2950 - accuracy: 0.9004\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "184/250 [=====================>........] - ETA: 36s - loss: 0.2944 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "185/250 [=====================>........] - ETA: 35s - loss: 0.2938 - accuracy: 0.9008\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "186/250 [=====================>........] - ETA: 35s - loss: 0.2931 - accuracy: 0.9010\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "187/250 [=====================>........] - ETA: 34s - loss: 0.2934 - accuracy: 0.9007\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "188/250 [=====================>........] - ETA: 34s - loss: 0.2942 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "189/250 [=====================>........] - ETA: 33s - loss: 0.2946 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "190/250 [=====================>........] - ETA: 33s - loss: 0.2949 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "191/250 [=====================>........] - ETA: 32s - loss: 0.2952 - accuracy: 0.9000\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "192/250 [======================>.......] - ETA: 31s - loss: 0.2950 - accuracy: 0.9001\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "193/250 [======================>.......] - ETA: 31s - loss: 0.2955 - accuracy: 0.8996\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "194/250 [======================>.......] - ETA: 30s - loss: 0.2963 - accuracy: 0.8993\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "195/250 [======================>.......] - ETA: 30s - loss: 0.2958 - accuracy: 0.8994\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "196/250 [======================>.......] - ETA: 29s - loss: 0.2957 - accuracy: 0.8992\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "197/250 [======================>.......] - ETA: 29s - loss: 0.2950 - accuracy: 0.8997\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "198/250 [======================>.......] - ETA: 28s - loss: 0.2955 - accuracy: 0.8995\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "199/250 [======================>.......] - ETA: 28s - loss: 0.2954 - accuracy: 0.8995\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "200/250 [=======================>......] - ETA: 27s - loss: 0.2949 - accuracy: 0.8997\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "201/250 [=======================>......] - ETA: 27s - loss: 0.2950 - accuracy: 0.8994\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "202/250 [=======================>......] - ETA: 26s - loss: 0.2945 - accuracy: 0.8996\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "203/250 [=======================>......] - ETA: 26s - loss: 0.2946 - accuracy: 0.8998\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "204/250 [=======================>......] - ETA: 25s - loss: 0.2949 - accuracy: 0.8997\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "205/250 [=======================>......] - ETA: 24s - loss: 0.2962 - accuracy: 0.8995\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "206/250 [=======================>......] - ETA: 24s - loss: 0.2964 - accuracy: 0.8997\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "207/250 [=======================>......] - ETA: 23s - loss: 0.2962 - accuracy: 0.8999\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "208/250 [=======================>......] - ETA: 23s - loss: 0.2963 - accuracy: 0.8999\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "209/250 [========================>.....] - ETA: 22s - loss: 0.2962 - accuracy: 0.9000\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "210/250 [========================>.....] - ETA: 22s - loss: 0.2963 - accuracy: 0.8999\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "211/250 [========================>.....] - ETA: 21s - loss: 0.2959 - accuracy: 0.9000\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "212/250 [========================>.....] - ETA: 20s - loss: 0.2954 - accuracy: 0.9004\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "213/250 [========================>.....] - ETA: 20s - loss: 0.2945 - accuracy: 0.9008\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "214/250 [========================>.....] - ETA: 19s - loss: 0.2953 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "215/250 [========================>.....] - ETA: 19s - loss: 0.2951 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "216/250 [========================>.....] - ETA: 18s - loss: 0.2955 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "217/250 [=========================>....] - ETA: 18s - loss: 0.2948 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "218/250 [=========================>....] - ETA: 17s - loss: 0.2955 - accuracy: 0.9004\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "219/250 [=========================>....] - ETA: 16s - loss: 0.2963 - accuracy: 0.9000\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "220/250 [=========================>....] - ETA: 16s - loss: 0.2957 - accuracy: 0.9000\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "221/250 [=========================>....] - ETA: 15s - loss: 0.2952 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "222/250 [=========================>....] - ETA: 15s - loss: 0.2955 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "223/250 [=========================>....] - ETA: 14s - loss: 0.2958 - accuracy: 0.9002\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "224/250 [=========================>....] - ETA: 14s - loss: 0.2958 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "225/250 [==========================>...] - ETA: 13s - loss: 0.2963 - accuracy: 0.9001\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "226/250 [==========================>...] - ETA: 13s - loss: 0.2972 - accuracy: 0.8996\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "227/250 [==========================>...] - ETA: 12s - loss: 0.2967 - accuracy: 0.8998\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "228/250 [==========================>...] - ETA: 12s - loss: 0.2971 - accuracy: 0.8998\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "229/250 [==========================>...] - ETA: 11s - loss: 0.2962 - accuracy: 0.9002\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "230/250 [==========================>...] - ETA: 11s - loss: 0.2954 - accuracy: 0.9004\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "231/250 [==========================>...] - ETA: 10s - loss: 0.2956 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "232/250 [==========================>...] - ETA: 9s - loss: 0.2950 - accuracy: 0.9003 \n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "233/250 [==========================>...] - ETA: 9s - loss: 0.2945 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "234/250 [===========================>..] - ETA: 8s - loss: 0.2943 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "235/250 [===========================>..] - ETA: 8s - loss: 0.2934 - accuracy: 0.9008\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "236/250 [===========================>..] - ETA: 7s - loss: 0.2935 - accuracy: 0.9010\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "237/250 [===========================>..] - ETA: 7s - loss: 0.2943 - accuracy: 0.9004\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "238/250 [===========================>..] - ETA: 6s - loss: 0.2953 - accuracy: 0.9002\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "239/250 [===========================>..] - ETA: 6s - loss: 0.2957 - accuracy: 0.9001\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "240/250 [===========================>..] - ETA: 5s - loss: 0.2949 - accuracy: 0.9004\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "241/250 [===========================>..] - ETA: 4s - loss: 0.2943 - accuracy: 0.9007\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "242/250 [============================>.] - ETA: 4s - loss: 0.2940 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "243/250 [============================>.] - ETA: 3s - loss: 0.2942 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "244/250 [============================>.] - ETA: 3s - loss: 0.2935 - accuracy: 0.9007\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "245/250 [============================>.] - ETA: 2s - loss: 0.2940 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "246/250 [============================>.] - ETA: 2s - loss: 0.2946 - accuracy: 0.9003\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "247/250 [============================>.] - ETA: 1s - loss: 0.2941 - accuracy: 0.9006\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "248/250 [============================>.] - ETA: 1s - loss: 0.2939 - accuracy: 0.9005\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.9007\n",
            "Epoch 4: accuracy did not improve from 0.95312\n",
            "250/250 [==============================] - 170s 677ms/step - loss: 0.2936 - accuracy: 0.9005 - val_loss: 0.1792 - val_accuracy: 0.9481\n",
            "Epoch 5/5\n",
            "\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "  1/250 [..............................] - ETA: 2:29 - loss: 0.2592 - accuracy: 0.9062\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "  2/250 [..............................] - ETA: 2:00 - loss: 0.2406 - accuracy: 0.8750\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "  3/250 [..............................] - ETA: 2:00 - loss: 0.2086 - accuracy: 0.9062\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "  4/250 [..............................] - ETA: 2:03 - loss: 0.2944 - accuracy: 0.8750\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "  5/250 [..............................] - ETA: 2:02 - loss: 0.3093 - accuracy: 0.8625\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "  6/250 [..............................] - ETA: 2:00 - loss: 0.2965 - accuracy: 0.8750\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "  7/250 [..............................] - ETA: 2:00 - loss: 0.2929 - accuracy: 0.8795\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "  8/250 [..............................] - ETA: 2:00 - loss: 0.2941 - accuracy: 0.8828\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "  9/250 [>.............................] - ETA: 2:00 - loss: 0.2883 - accuracy: 0.8854\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 10/250 [>.............................] - ETA: 1:59 - loss: 0.2947 - accuracy: 0.8813\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 11/250 [>.............................] - ETA: 1:58 - loss: 0.2935 - accuracy: 0.8807\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 12/250 [>.............................] - ETA: 1:58 - loss: 0.2828 - accuracy: 0.8880\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 13/250 [>.............................] - ETA: 1:58 - loss: 0.2802 - accuracy: 0.8918\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 14/250 [>.............................] - ETA: 1:57 - loss: 0.2842 - accuracy: 0.8929\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 15/250 [>.............................] - ETA: 1:56 - loss: 0.3021 - accuracy: 0.8813\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 16/250 [>.............................] - ETA: 1:58 - loss: 0.3059 - accuracy: 0.8809\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 17/250 [=>............................] - ETA: 2:01 - loss: 0.3032 - accuracy: 0.8860\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 18/250 [=>............................] - ETA: 2:04 - loss: 0.3003 - accuracy: 0.8889\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 19/250 [=>............................] - ETA: 2:06 - loss: 0.2919 - accuracy: 0.8931\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 20/250 [=>............................] - ETA: 2:07 - loss: 0.2840 - accuracy: 0.8969\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 21/250 [=>............................] - ETA: 2:07 - loss: 0.2762 - accuracy: 0.8988\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 22/250 [=>............................] - ETA: 2:06 - loss: 0.2697 - accuracy: 0.9020\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 23/250 [=>............................] - ETA: 2:04 - loss: 0.2700 - accuracy: 0.9008\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 24/250 [=>............................] - ETA: 2:03 - loss: 0.2685 - accuracy: 0.9036\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 25/250 [==>...........................] - ETA: 2:02 - loss: 0.2672 - accuracy: 0.9038\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 26/250 [==>...........................] - ETA: 2:01 - loss: 0.2690 - accuracy: 0.9026\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 27/250 [==>...........................] - ETA: 2:00 - loss: 0.2648 - accuracy: 0.9039\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 28/250 [==>...........................] - ETA: 1:59 - loss: 0.2681 - accuracy: 0.9007\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 29/250 [==>...........................] - ETA: 1:59 - loss: 0.2671 - accuracy: 0.9019\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 30/250 [==>...........................] - ETA: 1:58 - loss: 0.2645 - accuracy: 0.9042\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 31/250 [==>...........................] - ETA: 1:57 - loss: 0.2625 - accuracy: 0.9052\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 32/250 [==>...........................] - ETA: 1:56 - loss: 0.2665 - accuracy: 0.9014\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 33/250 [==>...........................] - ETA: 1:55 - loss: 0.2630 - accuracy: 0.9044\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 34/250 [===>..........................] - ETA: 1:54 - loss: 0.2620 - accuracy: 0.9035\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 35/250 [===>..........................] - ETA: 1:54 - loss: 0.2631 - accuracy: 0.9018\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 36/250 [===>..........................] - ETA: 1:53 - loss: 0.2590 - accuracy: 0.9045\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 37/250 [===>..........................] - ETA: 1:52 - loss: 0.2581 - accuracy: 0.9062\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 38/250 [===>..........................] - ETA: 1:51 - loss: 0.2561 - accuracy: 0.9079\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 39/250 [===>..........................] - ETA: 1:51 - loss: 0.2539 - accuracy: 0.9087\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 40/250 [===>..........................] - ETA: 1:50 - loss: 0.2553 - accuracy: 0.9094\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 41/250 [===>..........................] - ETA: 1:50 - loss: 0.2539 - accuracy: 0.9116\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 42/250 [====>.........................] - ETA: 1:50 - loss: 0.2578 - accuracy: 0.9115\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 43/250 [====>.........................] - ETA: 1:50 - loss: 0.2578 - accuracy: 0.9113\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 44/250 [====>.........................] - ETA: 1:51 - loss: 0.2580 - accuracy: 0.9112\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 45/250 [====>.........................] - ETA: 1:51 - loss: 0.2595 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 46/250 [====>.........................] - ETA: 1:51 - loss: 0.2582 - accuracy: 0.9110\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 47/250 [====>.........................] - ETA: 1:50 - loss: 0.2568 - accuracy: 0.9122\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 48/250 [====>.........................] - ETA: 1:50 - loss: 0.2552 - accuracy: 0.9134\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 49/250 [====>.........................] - ETA: 1:49 - loss: 0.2588 - accuracy: 0.9107\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 50/250 [=====>........................] - ETA: 1:48 - loss: 0.2629 - accuracy: 0.9081\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 51/250 [=====>........................] - ETA: 1:47 - loss: 0.2635 - accuracy: 0.9093\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 52/250 [=====>........................] - ETA: 1:46 - loss: 0.2613 - accuracy: 0.9105\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 53/250 [=====>........................] - ETA: 1:46 - loss: 0.2606 - accuracy: 0.9116\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 54/250 [=====>........................] - ETA: 1:45 - loss: 0.2577 - accuracy: 0.9132\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 55/250 [=====>........................] - ETA: 1:44 - loss: 0.2573 - accuracy: 0.9136\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 56/250 [=====>........................] - ETA: 1:43 - loss: 0.2575 - accuracy: 0.9129\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 57/250 [=====>........................] - ETA: 1:43 - loss: 0.2569 - accuracy: 0.9128\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 58/250 [=====>........................] - ETA: 1:42 - loss: 0.2567 - accuracy: 0.9127\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 59/250 [======>.......................] - ETA: 1:42 - loss: 0.2554 - accuracy: 0.9126\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 60/250 [======>.......................] - ETA: 1:41 - loss: 0.2557 - accuracy: 0.9125\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 61/250 [======>.......................] - ETA: 1:40 - loss: 0.2579 - accuracy: 0.9119\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 62/250 [======>.......................] - ETA: 1:40 - loss: 0.2552 - accuracy: 0.9128\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 63/250 [======>.......................] - ETA: 1:39 - loss: 0.2568 - accuracy: 0.9132\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 64/250 [======>.......................] - ETA: 1:38 - loss: 0.2551 - accuracy: 0.9136\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 65/250 [======>.......................] - ETA: 1:38 - loss: 0.2563 - accuracy: 0.9135\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 66/250 [======>.......................] - ETA: 1:37 - loss: 0.2577 - accuracy: 0.9119\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 67/250 [=======>......................] - ETA: 1:37 - loss: 0.2592 - accuracy: 0.9114\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 68/250 [=======>......................] - ETA: 1:37 - loss: 0.2594 - accuracy: 0.9122\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 69/250 [=======>......................] - ETA: 1:37 - loss: 0.2606 - accuracy: 0.9117\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 70/250 [=======>......................] - ETA: 1:37 - loss: 0.2602 - accuracy: 0.9125\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 71/250 [=======>......................] - ETA: 1:37 - loss: 0.2582 - accuracy: 0.9137\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 72/250 [=======>......................] - ETA: 1:37 - loss: 0.2576 - accuracy: 0.9145\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 73/250 [=======>......................] - ETA: 1:36 - loss: 0.2576 - accuracy: 0.9140\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 74/250 [=======>......................] - ETA: 1:35 - loss: 0.2584 - accuracy: 0.9134\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 75/250 [========>.....................] - ETA: 1:35 - loss: 0.2580 - accuracy: 0.9129\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 76/250 [========>.....................] - ETA: 1:34 - loss: 0.2627 - accuracy: 0.9128\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 77/250 [========>.....................] - ETA: 1:33 - loss: 0.2611 - accuracy: 0.9131\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 78/250 [========>.....................] - ETA: 1:33 - loss: 0.2635 - accuracy: 0.9123\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 79/250 [========>.....................] - ETA: 1:32 - loss: 0.2627 - accuracy: 0.9126\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 80/250 [========>.....................] - ETA: 1:31 - loss: 0.2632 - accuracy: 0.9129\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 81/250 [========>.....................] - ETA: 1:31 - loss: 0.2654 - accuracy: 0.9124\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 82/250 [========>.....................] - ETA: 1:30 - loss: 0.2654 - accuracy: 0.9127\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 83/250 [========>.....................] - ETA: 1:30 - loss: 0.2648 - accuracy: 0.9127\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 84/250 [=========>....................] - ETA: 1:29 - loss: 0.2643 - accuracy: 0.9129\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 85/250 [=========>....................] - ETA: 1:28 - loss: 0.2623 - accuracy: 0.9140\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 86/250 [=========>....................] - ETA: 1:28 - loss: 0.2612 - accuracy: 0.9142\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 87/250 [=========>....................] - ETA: 1:28 - loss: 0.2600 - accuracy: 0.9145\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 88/250 [=========>....................] - ETA: 1:28 - loss: 0.2612 - accuracy: 0.9141\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 89/250 [=========>....................] - ETA: 1:28 - loss: 0.2616 - accuracy: 0.9140\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 90/250 [=========>....................] - ETA: 1:27 - loss: 0.2613 - accuracy: 0.9142\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 91/250 [=========>....................] - ETA: 1:27 - loss: 0.2592 - accuracy: 0.9148\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 92/250 [==========>...................] - ETA: 1:27 - loss: 0.2594 - accuracy: 0.9147\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 93/250 [==========>...................] - ETA: 1:27 - loss: 0.2586 - accuracy: 0.9150\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 94/250 [==========>...................] - ETA: 1:26 - loss: 0.2594 - accuracy: 0.9146\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 95/250 [==========>...................] - ETA: 1:26 - loss: 0.2608 - accuracy: 0.9135\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 96/250 [==========>...................] - ETA: 1:25 - loss: 0.2615 - accuracy: 0.9128\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 97/250 [==========>...................] - ETA: 1:25 - loss: 0.2605 - accuracy: 0.9130\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 98/250 [==========>...................] - ETA: 1:24 - loss: 0.2599 - accuracy: 0.9126\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            " 99/250 [==========>...................] - ETA: 1:23 - loss: 0.2617 - accuracy: 0.9119\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "100/250 [===========>..................] - ETA: 1:23 - loss: 0.2622 - accuracy: 0.9116\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "101/250 [===========>..................] - ETA: 1:22 - loss: 0.2626 - accuracy: 0.9112\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "102/250 [===========>..................] - ETA: 1:21 - loss: 0.2635 - accuracy: 0.9112\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "103/250 [===========>..................] - ETA: 1:21 - loss: 0.2632 - accuracy: 0.9108\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "104/250 [===========>..................] - ETA: 1:20 - loss: 0.2637 - accuracy: 0.9108\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "105/250 [===========>..................] - ETA: 1:19 - loss: 0.2634 - accuracy: 0.9107\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "106/250 [===========>..................] - ETA: 1:19 - loss: 0.2633 - accuracy: 0.9110\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "107/250 [===========>..................] - ETA: 1:18 - loss: 0.2620 - accuracy: 0.9115\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "108/250 [===========>..................] - ETA: 1:17 - loss: 0.2612 - accuracy: 0.9117\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "109/250 [============>.................] - ETA: 1:17 - loss: 0.2611 - accuracy: 0.9117\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "110/250 [============>.................] - ETA: 1:16 - loss: 0.2622 - accuracy: 0.9114\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "111/250 [============>.................] - ETA: 1:16 - loss: 0.2623 - accuracy: 0.9110\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "112/250 [============>.................] - ETA: 1:15 - loss: 0.2613 - accuracy: 0.9113\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "113/250 [============>.................] - ETA: 1:14 - loss: 0.2601 - accuracy: 0.9115\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "114/250 [============>.................] - ETA: 1:14 - loss: 0.2599 - accuracy: 0.9112\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "115/250 [============>.................] - ETA: 1:13 - loss: 0.2589 - accuracy: 0.9117\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "116/250 [============>.................] - ETA: 1:13 - loss: 0.2579 - accuracy: 0.9116\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "117/250 [=============>................] - ETA: 1:12 - loss: 0.2581 - accuracy: 0.9113\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "118/250 [=============>................] - ETA: 1:12 - loss: 0.2580 - accuracy: 0.9113\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "119/250 [=============>................] - ETA: 1:12 - loss: 0.2576 - accuracy: 0.9110\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "120/250 [=============>................] - ETA: 1:11 - loss: 0.2591 - accuracy: 0.9107\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "121/250 [=============>................] - ETA: 1:11 - loss: 0.2590 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "122/250 [=============>................] - ETA: 1:10 - loss: 0.2589 - accuracy: 0.9106\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "123/250 [=============>................] - ETA: 1:09 - loss: 0.2592 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "124/250 [=============>................] - ETA: 1:09 - loss: 0.2597 - accuracy: 0.9105\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "125/250 [==============>...............] - ETA: 1:08 - loss: 0.2593 - accuracy: 0.9107\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "126/250 [==============>...............] - ETA: 1:08 - loss: 0.2579 - accuracy: 0.9112\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "127/250 [==============>...............] - ETA: 1:07 - loss: 0.2582 - accuracy: 0.9107\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "128/250 [==============>...............] - ETA: 1:06 - loss: 0.2583 - accuracy: 0.9109\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "129/250 [==============>...............] - ETA: 1:06 - loss: 0.2581 - accuracy: 0.9109\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "130/250 [==============>...............] - ETA: 1:05 - loss: 0.2585 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "131/250 [==============>...............] - ETA: 1:05 - loss: 0.2585 - accuracy: 0.9105\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "132/250 [==============>...............] - ETA: 1:04 - loss: 0.2578 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "133/250 [==============>...............] - ETA: 1:03 - loss: 0.2581 - accuracy: 0.9102\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "134/250 [===============>..............] - ETA: 1:03 - loss: 0.2585 - accuracy: 0.9102\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "135/250 [===============>..............] - ETA: 1:02 - loss: 0.2583 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "136/250 [===============>..............] - ETA: 1:02 - loss: 0.2588 - accuracy: 0.9099\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "137/250 [===============>..............] - ETA: 1:01 - loss: 0.2588 - accuracy: 0.9099\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "138/250 [===============>..............] - ETA: 1:00 - loss: 0.2583 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "139/250 [===============>..............] - ETA: 1:00 - loss: 0.2582 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "140/250 [===============>..............] - ETA: 59s - loss: 0.2583 - accuracy: 0.9100 \n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "141/250 [===============>..............] - ETA: 59s - loss: 0.2575 - accuracy: 0.9105\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "142/250 [================>.............] - ETA: 58s - loss: 0.2575 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "143/250 [================>.............] - ETA: 58s - loss: 0.2574 - accuracy: 0.9102\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "144/250 [================>.............] - ETA: 58s - loss: 0.2569 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "145/250 [================>.............] - ETA: 57s - loss: 0.2573 - accuracy: 0.9097\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "146/250 [================>.............] - ETA: 57s - loss: 0.2577 - accuracy: 0.9097\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "147/250 [================>.............] - ETA: 56s - loss: 0.2573 - accuracy: 0.9099\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "148/250 [================>.............] - ETA: 55s - loss: 0.2581 - accuracy: 0.9094\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "149/250 [================>.............] - ETA: 55s - loss: 0.2583 - accuracy: 0.9094\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "150/250 [=================>............] - ETA: 54s - loss: 0.2585 - accuracy: 0.9098\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "151/250 [=================>............] - ETA: 54s - loss: 0.2584 - accuracy: 0.9098\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "152/250 [=================>............] - ETA: 53s - loss: 0.2583 - accuracy: 0.9100\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "153/250 [=================>............] - ETA: 53s - loss: 0.2589 - accuracy: 0.9099\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "154/250 [=================>............] - ETA: 52s - loss: 0.2582 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "155/250 [=================>............] - ETA: 51s - loss: 0.2584 - accuracy: 0.9101\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "156/250 [=================>............] - ETA: 51s - loss: 0.2580 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "157/250 [=================>............] - ETA: 50s - loss: 0.2594 - accuracy: 0.9096\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "158/250 [=================>............] - ETA: 50s - loss: 0.2607 - accuracy: 0.9092\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "159/250 [==================>...........] - ETA: 49s - loss: 0.2611 - accuracy: 0.9088\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "160/250 [==================>...........] - ETA: 48s - loss: 0.2601 - accuracy: 0.9092\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "161/250 [==================>...........] - ETA: 48s - loss: 0.2592 - accuracy: 0.9095\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "162/250 [==================>...........] - ETA: 47s - loss: 0.2597 - accuracy: 0.9091\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "163/250 [==================>...........] - ETA: 47s - loss: 0.2590 - accuracy: 0.9095\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "164/250 [==================>...........] - ETA: 46s - loss: 0.2602 - accuracy: 0.9097\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "165/250 [==================>...........] - ETA: 46s - loss: 0.2600 - accuracy: 0.9095\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "166/250 [==================>...........] - ETA: 45s - loss: 0.2601 - accuracy: 0.9096\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "167/250 [===================>..........] - ETA: 45s - loss: 0.2593 - accuracy: 0.9100\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "168/250 [===================>..........] - ETA: 44s - loss: 0.2593 - accuracy: 0.9100\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "169/250 [===================>..........] - ETA: 44s - loss: 0.2607 - accuracy: 0.9092\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "170/250 [===================>..........] - ETA: 43s - loss: 0.2617 - accuracy: 0.9086\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "171/250 [===================>..........] - ETA: 43s - loss: 0.2625 - accuracy: 0.9079\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "172/250 [===================>..........] - ETA: 42s - loss: 0.2631 - accuracy: 0.9079\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "173/250 [===================>..........] - ETA: 42s - loss: 0.2623 - accuracy: 0.9082\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "174/250 [===================>..........] - ETA: 41s - loss: 0.2616 - accuracy: 0.9084\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "175/250 [====================>.........] - ETA: 41s - loss: 0.2633 - accuracy: 0.9082\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "176/250 [====================>.........] - ETA: 40s - loss: 0.2628 - accuracy: 0.9084\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "177/250 [====================>.........] - ETA: 39s - loss: 0.2621 - accuracy: 0.9085\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "178/250 [====================>.........] - ETA: 39s - loss: 0.2628 - accuracy: 0.9084\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "179/250 [====================>.........] - ETA: 38s - loss: 0.2647 - accuracy: 0.9082\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "180/250 [====================>.........] - ETA: 38s - loss: 0.2652 - accuracy: 0.9082\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "181/250 [====================>.........] - ETA: 37s - loss: 0.2643 - accuracy: 0.9087\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "182/250 [====================>.........] - ETA: 37s - loss: 0.2642 - accuracy: 0.9087\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "183/250 [====================>.........] - ETA: 36s - loss: 0.2652 - accuracy: 0.9085\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "184/250 [=====================>........] - ETA: 36s - loss: 0.2650 - accuracy: 0.9086\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "185/250 [=====================>........] - ETA: 35s - loss: 0.2659 - accuracy: 0.9086\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "186/250 [=====================>........] - ETA: 34s - loss: 0.2655 - accuracy: 0.9086\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "187/250 [=====================>........] - ETA: 34s - loss: 0.2666 - accuracy: 0.9084\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "188/250 [=====================>........] - ETA: 33s - loss: 0.2662 - accuracy: 0.9086\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "189/250 [=====================>........] - ETA: 33s - loss: 0.2658 - accuracy: 0.9087\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "190/250 [=====================>........] - ETA: 32s - loss: 0.2657 - accuracy: 0.9086\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "191/250 [=====================>........] - ETA: 32s - loss: 0.2653 - accuracy: 0.9087\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "192/250 [======================>.......] - ETA: 31s - loss: 0.2649 - accuracy: 0.9089\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "193/250 [======================>.......] - ETA: 30s - loss: 0.2640 - accuracy: 0.9092\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "194/250 [======================>.......] - ETA: 30s - loss: 0.2633 - accuracy: 0.9097\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "195/250 [======================>.......] - ETA: 29s - loss: 0.2644 - accuracy: 0.9092\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "196/250 [======================>.......] - ETA: 29s - loss: 0.2645 - accuracy: 0.9091\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "197/250 [======================>.......] - ETA: 28s - loss: 0.2639 - accuracy: 0.9095\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "198/250 [======================>.......] - ETA: 28s - loss: 0.2631 - accuracy: 0.9098\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "199/250 [======================>.......] - ETA: 27s - loss: 0.2625 - accuracy: 0.9099\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "200/250 [=======================>......] - ETA: 27s - loss: 0.2616 - accuracy: 0.9102\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "201/250 [=======================>......] - ETA: 26s - loss: 0.2624 - accuracy: 0.9102\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "202/250 [=======================>......] - ETA: 26s - loss: 0.2625 - accuracy: 0.9101\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "203/250 [=======================>......] - ETA: 25s - loss: 0.2622 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "204/250 [=======================>......] - ETA: 25s - loss: 0.2618 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "205/250 [=======================>......] - ETA: 24s - loss: 0.2620 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "206/250 [=======================>......] - ETA: 23s - loss: 0.2617 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "207/250 [=======================>......] - ETA: 23s - loss: 0.2622 - accuracy: 0.9102\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "208/250 [=======================>......] - ETA: 22s - loss: 0.2616 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "209/250 [========================>.....] - ETA: 22s - loss: 0.2612 - accuracy: 0.9105\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "210/250 [========================>.....] - ETA: 21s - loss: 0.2617 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "211/250 [========================>.....] - ETA: 21s - loss: 0.2614 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "212/250 [========================>.....] - ETA: 20s - loss: 0.2609 - accuracy: 0.9106\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "213/250 [========================>.....] - ETA: 20s - loss: 0.2602 - accuracy: 0.9108\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "214/250 [========================>.....] - ETA: 19s - loss: 0.2601 - accuracy: 0.9110\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "215/250 [========================>.....] - ETA: 18s - loss: 0.2610 - accuracy: 0.9105\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "216/250 [========================>.....] - ETA: 18s - loss: 0.2609 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "217/250 [=========================>....] - ETA: 17s - loss: 0.2617 - accuracy: 0.9100\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "218/250 [=========================>....] - ETA: 17s - loss: 0.2612 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "219/250 [=========================>....] - ETA: 16s - loss: 0.2620 - accuracy: 0.9100\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "220/250 [=========================>....] - ETA: 16s - loss: 0.2614 - accuracy: 0.9101\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "221/250 [=========================>....] - ETA: 15s - loss: 0.2612 - accuracy: 0.9102\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "222/250 [=========================>....] - ETA: 15s - loss: 0.2606 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "223/250 [=========================>....] - ETA: 14s - loss: 0.2606 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "224/250 [=========================>....] - ETA: 14s - loss: 0.2602 - accuracy: 0.9105\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "225/250 [==========================>...] - ETA: 13s - loss: 0.2602 - accuracy: 0.9106\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "226/250 [==========================>...] - ETA: 13s - loss: 0.2602 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "227/250 [==========================>...] - ETA: 12s - loss: 0.2604 - accuracy: 0.9105\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "228/250 [==========================>...] - ETA: 11s - loss: 0.2607 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "229/250 [==========================>...] - ETA: 11s - loss: 0.2611 - accuracy: 0.9104\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "230/250 [==========================>...] - ETA: 10s - loss: 0.2614 - accuracy: 0.9102\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "231/250 [==========================>...] - ETA: 10s - loss: 0.2611 - accuracy: 0.9103\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "232/250 [==========================>...] - ETA: 9s - loss: 0.2606 - accuracy: 0.9106 \n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "233/250 [==========================>...] - ETA: 9s - loss: 0.2600 - accuracy: 0.9107\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "234/250 [===========================>..] - ETA: 8s - loss: 0.2600 - accuracy: 0.9108\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "235/250 [===========================>..] - ETA: 8s - loss: 0.2596 - accuracy: 0.9109\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "236/250 [===========================>..] - ETA: 7s - loss: 0.2593 - accuracy: 0.9112\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "237/250 [===========================>..] - ETA: 7s - loss: 0.2598 - accuracy: 0.9109\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "238/250 [===========================>..] - ETA: 6s - loss: 0.2596 - accuracy: 0.9109\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "239/250 [===========================>..] - ETA: 5s - loss: 0.2596 - accuracy: 0.9109\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "240/250 [===========================>..] - ETA: 5s - loss: 0.2590 - accuracy: 0.9111\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "241/250 [===========================>..] - ETA: 4s - loss: 0.2588 - accuracy: 0.9112\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "242/250 [============================>.] - ETA: 4s - loss: 0.2591 - accuracy: 0.9112\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "243/250 [============================>.] - ETA: 3s - loss: 0.2592 - accuracy: 0.9110\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "244/250 [============================>.] - ETA: 3s - loss: 0.2588 - accuracy: 0.9111\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "245/250 [============================>.] - ETA: 2s - loss: 0.2591 - accuracy: 0.9110\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "246/250 [============================>.] - ETA: 2s - loss: 0.2592 - accuracy: 0.9109\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "247/250 [============================>.] - ETA: 1s - loss: 0.2596 - accuracy: 0.9107\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "248/250 [============================>.] - ETA: 1s - loss: 0.2593 - accuracy: 0.9108\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.2590 - accuracy: 0.9112\n",
            "Epoch 5: accuracy did not improve from 0.95312\n",
            "250/250 [==============================] - 168s 671ms/step - loss: 0.2591 - accuracy: 0.9110 - val_loss: 0.1555 - val_accuracy: 0.9526\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit(\n",
        "    trainGen, steps_per_epoch=trainGen.samples // 32,\n",
        "\tvalidation_data=testGen, validation_steps=testGen.samples // 32,\n",
        "\tepochs=5, callbacks=[checkpoint, early])\n",
        "\n",
        "model.save(os.path.sep.join([OUTPUT_PATH, \"resnet101.model\"]), save_format='h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BDFuf4_RyDRy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "outputId": "71b67375-0d4f-4d60-dee6-5d4b4d7d1889"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "N = np.arange(0, 5)\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(N, hist.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, hist.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.sep.join([OUTPUT_PATH, \"resnet_losses.png\"]))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(N, hist.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(N, hist.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.sep.join([OUTPUT_PATH, \"resnet_accuracy.png\"]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vKBRYly7yDRz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(os.path.sep.join([OUTPUT_PATH, \"resnet101.model\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sAQw1bN9yDRz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1290d55a-b0d4-4703-9aeb-fcfe6646cd7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        open      0.808     0.974     0.883       388\n",
            "       short      0.822     0.967     0.889       301\n",
            "    mousebit      0.910     0.697     0.790       393\n",
            "        spur      0.948     0.778     0.855       325\n",
            "      copper      0.993     0.925     0.958       294\n",
            "    pin-hole      0.884     0.993     0.936       300\n",
            "\n",
            "    accuracy                          0.883      2001\n",
            "   macro avg      0.894     0.889     0.885      2001\n",
            "weighted avg      0.891     0.883     0.880      2001\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "predictions = list()\n",
        "for path in testGen.filepaths:\n",
        "    image = cv2.imread(path)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    preds = model.predict(image)\n",
        "    predictions.append(preds.argmax(axis=1))\n",
        "\n",
        "print(classification_report(testGen.classes,\n",
        "\tpredictions, target_names=testGen.class_indices, digits=3))\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.grid(False)\n",
        "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
        "font = {'size' : 12}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "cm = confusion_matrix(testGen.classes, predictions)\n",
        "\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(testGen.class_indices))\n",
        "\n",
        "plt.xticks(tick_marks, testGen.class_indices, rotation=45)\n",
        "plt.yticks(tick_marks, testGen.class_indices)\n",
        "\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.xlabel('Predicted Defect')\n",
        "plt.ylabel('True Defect')\n",
        "plt.title('Confusion matrix ResNet-101')\n",
        "plt.savefig(OUTPUT_PATH + '/resnet_confusion_matrix.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR2mlfLX41kq"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/Train_PCB_faults/\n",
        "# model_json =model.to_json()\n",
        "# with open(\"model_resnet101.json\",\"w\") as json_file:\n",
        "#   json_file.write(model_json)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "25c84cac0d85cf2b51a97d2bd7b0fa6585599b14f00604962e71b8ec1671851d"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}